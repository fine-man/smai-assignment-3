{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - SMAI - DoubleMNIST and PermutedMnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vanshg/play/IIIITH/sem5/smai/assignments/assignment-3-fine-man/datasets\n",
      "/home/vanshg/play/IIIITH/sem5/smai/assignments/assignment-3-fine-man/yaml-files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# enter the Foldername here:\n",
    "FOLDERNAME = \"/home/richard/play/IIITH/sem5/smai/assignments/assignment-3-fine-man\"\n",
    "\n",
    "if FOLDERNAME is None or not os.path.exists(FOLDERNAME):\n",
    "    FOLDERNAME = os.getcwd()\n",
    "\n",
    "PATHNAME = f\"{FOLDERNAME}\"\n",
    "sys.path.append(f\"{FOLDERNAME}\")\n",
    "\n",
    "# DATA_FOLDER = os.path.join(FOLDERNAME, \"SMAI-Dataset-release/IIIT-CFW\")\n",
    "DATA_FOLDER = os.path.join(FOLDERNAME, \"datasets\")\n",
    "YAML_FOLDER = os.path.join(FOLDERNAME, \"yaml-files\")\n",
    "print(DATA_FOLDER)\n",
    "print(YAML_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some magic so that the notebook will reload external python modules;\n",
    "# see https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import scipy\n",
    "import scipy.io\n",
    "import os\n",
    "from random import randrange\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_torch import *\n",
    "from src_torch.classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanshg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.1) Multi-digit Recognition on Multi-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1.0) Data Loading, Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training data: 64000\n",
      "Length of Validation data: 16000\n",
      "Length of Testing Data: 20000\n"
     ]
    }
   ],
   "source": [
    "DOUBLE_MNIST_FOLDER = os.path.join(DATA_FOLDER, \"DOUBLE_MNIST\")\n",
    "\n",
    "train_img_paths = []\n",
    "val_img_paths = []\n",
    "test_img_paths = []\n",
    "\n",
    "# Saving path of all images in train set\n",
    "for img_folder in glob.glob(os.path.join(DOUBLE_MNIST_FOLDER, \"train/*\")):\n",
    "    label = int(os.path.basename(img_folder))\n",
    "    for img_file in glob.glob(os.path.join(img_folder, \"*\")):\n",
    "        train_img_paths.append(img_file)\n",
    "\n",
    "# Saving path of all images in val set\n",
    "for img_folder in glob.glob(os.path.join(DOUBLE_MNIST_FOLDER, \"val/*\")):\n",
    "    label = int(os.path.basename(img_folder))\n",
    "    for img_file in glob.glob(os.path.join(img_folder, \"*\")):\n",
    "        val_img_paths.append(img_file)\n",
    "\n",
    "# Saving path of all images in test set\n",
    "for img_folder in glob.glob(os.path.join(DOUBLE_MNIST_FOLDER, \"test/*\")):\n",
    "    label = int(os.path.basename(img_folder))\n",
    "    for img_file in glob.glob(os.path.join(img_folder, \"*\")):\n",
    "        test_img_paths.append(img_file)\n",
    "\n",
    "print(f\"Length of Training data: {len(train_img_paths)}\")\n",
    "print(f\"Length of Validation data: {len(val_img_paths)}\")\n",
    "print(f\"Length of Testing Data: {len(test_img_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DoubleMNIST(train_img_paths)\n",
    "val_dataset = DoubleMNIST(val_img_paths)\n",
    "test_dataset = DoubleMNIST(test_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.2) - Permuted MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2.0) Data Loading, PreProcessing and Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "PERMUTED_MNIST_FILE = os.path.join(DATA_FOLDER, \"permuted_mnist.npz\")\n",
    "\n",
    "data = np.load(PERMUTED_MNIST_FILE)\n",
    "\n",
    "train_images = torch.tensor(data['train_images'])/255.0\n",
    "train_images = train_images.unsqueeze(1)\n",
    "train_labels = torch.tensor(data['train_labels'])\n",
    "test_images = torch.tensor(data['test_images'])/255.0\n",
    "test_images = test_images.unsqueeze(1)\n",
    "test_labels = torch.tensor(data['test_labels'])\n",
    "\n",
    "complete_train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into Train/Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data: 42000\n",
      "Length of Validation Data: 18000\n",
      "Length of Testing Data: 10000\n"
     ]
    }
   ],
   "source": [
    "val_ratio = 0.3\n",
    "val_size = int(val_ratio * len(complete_train_dataset))\n",
    "train_size = len(complete_train_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(complete_train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Training Data: {len(train_dataset)}\")\n",
    "print(f\"Length of Validation Data: {len(val_dataset)}\")\n",
    "print(f\"Length of Testing Data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5.2.1) - MLP on Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            hidden_dims,\n",
    "            num_classes,\n",
    "            flatten_first=False, # Whether to flatten the input first\n",
    "            last_activation=False, # activation to be used after last layer\n",
    "    ):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = len(hidden_dims) + 1\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.flatten_first = flatten_first\n",
    "        self.last_activation = last_activation \n",
    "        modules = []\n",
    "\n",
    "        # first layer\n",
    "        in_dim = input_dim\n",
    "        if self.num_layers > 1:\n",
    "            output_dim = hidden_dims[0]\n",
    "            modules.append(nn.Linear(in_dim, output_dim))\n",
    "            modules.append(nn.ReLU())\n",
    "            in_dim = output_dim\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            out_dim = hidden_dims[i]\n",
    "            modules.append(nn.Linear(in_dim, out_dim))\n",
    "            modules.append(nn.ReLU())\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        # Last Layer\n",
    "        out_dim = num_classes\n",
    "        modules.append(nn.Linear(in_dim, out_dim))\n",
    "        if self.last_activation:\n",
    "            modules.append(nn.ReLU())\n",
    "\n",
    "        self.linears = nn.ModuleList(modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0] # Number of samples\n",
    "        if self.flatten_first:\n",
    "            out = x.reshape(N, -1)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        for module in self.linears:\n",
    "            out = module(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model is on device: cuda\n",
      "\n",
      "Number of Iterations Per Epoch: 420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100/4200 | loss = 0.4231\n",
      "Iteration: 200/4200 | loss = 0.3212\n",
      "Iteration: 300/4200 | loss = 0.2445\n",
      "Iteration: 400/4200 | loss = 0.4077\n",
      "Epoch: 1 | Train Accuracy: 92.005 | Val Accuracy: 91.333|  Train loss: 0.2873 | Val loss: 0.3076\n",
      "\n",
      "Iteration: 500/4200 | loss = 0.4777\n",
      "Iteration: 600/4200 | loss = 0.3131\n",
      "Iteration: 700/4200 | loss = 0.2633\n",
      "Iteration: 800/4200 | loss = 0.2678\n",
      "Epoch: 2 | Train Accuracy: 92.074 | Val Accuracy: 90.894|  Train loss: 0.2826 | Val loss: 0.3192\n",
      "\n",
      "Iteration: 900/4200 | loss = 0.3209\n",
      "Iteration: 1000/4200 | loss = 0.3706\n",
      "Iteration: 1100/4200 | loss = 0.3235\n",
      "Iteration: 1200/4200 | loss = 0.3436\n",
      "Epoch: 3 | Train Accuracy: 92.748 | Val Accuracy: 91.667|  Train loss: 0.2623 | Val loss: 0.3046\n",
      "\n",
      "Iteration: 1300/4200 | loss = 0.2735\n",
      "Iteration: 1400/4200 | loss = 0.1594\n",
      "Iteration: 1500/4200 | loss = 0.2525\n",
      "Iteration: 1600/4200 | loss = 0.2847\n",
      "Epoch: 4 | Train Accuracy: 92.440 | Val Accuracy: 90.972|  Train loss: 0.2598 | Val loss: 0.3190\n",
      "\n",
      "Iteration: 1700/4200 | loss = 0.1982\n",
      "Iteration: 1800/4200 | loss = 0.3647\n",
      "Iteration: 1900/4200 | loss = 0.2233\n",
      "Iteration: 2000/4200 | loss = 0.4773\n",
      "Iteration: 2100/4200 | loss = 0.2445\n",
      "Epoch: 5 | Train Accuracy: 92.852 | Val Accuracy: 91.328|  Train loss: 0.2554 | Val loss: 0.3131\n",
      "\n",
      "Iteration: 2200/4200 | loss = 0.1447\n",
      "Iteration: 2300/4200 | loss = 0.2409\n",
      "Iteration: 2400/4200 | loss = 0.4425\n",
      "Iteration: 2500/4200 | loss = 0.3449\n",
      "Epoch: 6 | Train Accuracy: 93.086 | Val Accuracy: 91.511|  Train loss: 0.2455 | Val loss: 0.3129\n",
      "\n",
      "Iteration: 2600/4200 | loss = 0.2719\n",
      "Iteration: 2700/4200 | loss = 0.3341\n",
      "Iteration: 2800/4200 | loss = 0.2278\n",
      "Iteration: 2900/4200 | loss = 0.3124\n",
      "Epoch: 7 | Train Accuracy: 93.081 | Val Accuracy: 91.467|  Train loss: 0.2447 | Val loss: 0.3176\n",
      "\n",
      "Iteration: 3000/4200 | loss = 0.3121\n",
      "Iteration: 3100/4200 | loss = 0.1704\n",
      "Iteration: 3200/4200 | loss = 0.1268\n",
      "Iteration: 3300/4200 | loss = 0.2573\n",
      "Epoch: 8 | Train Accuracy: 92.912 | Val Accuracy: 91.228|  Train loss: 0.2454 | Val loss: 0.3246\n",
      "\n",
      "Iteration: 3400/4200 | loss = 0.2631\n",
      "Iteration: 3500/4200 | loss = 0.2318\n",
      "Iteration: 3600/4200 | loss = 0.2684\n",
      "Iteration: 3700/4200 | loss = 0.3785\n",
      "Epoch: 9 | Train Accuracy: 92.410 | Val Accuracy: 90.867|  Train loss: 0.2634 | Val loss: 0.3424\n",
      "\n",
      "Iteration: 3800/4200 | loss = 0.2864\n",
      "Iteration: 3900/4200 | loss = 0.2467\n",
      "Iteration: 4000/4200 | loss = 0.2801\n",
      "Iteration: 4100/4200 | loss = 0.1264\n",
      "Iteration: 4200/4200 | loss = 0.5198\n",
      "Epoch: 10 | Train Accuracy: 92.871 | Val Accuracy: 91.211|  Train loss: 0.2482 | Val loss: 0.3307\n",
      "\n",
      "\n",
      "BEST VAL ACCURACY : 91.6667 | Best Epoch: 3 | Val loss: 0.3046\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"input_dim\": 28*28,\n",
    "    \"hidden_dims\": [],\n",
    "    \"num_classes\": 10,\n",
    "    \"flatten_first\": True\n",
    "}\n",
    "\n",
    "mlp_model = SimpleMLP(**model_config)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)\n",
    "\n",
    "print(mlp_model)\n",
    "\n",
    "train_config = {\n",
    "    \"device\": device,\n",
    "    \"print_every\": 100\n",
    "}\n",
    "\n",
    "_ = train(mlp_model, criterion, optimizer, train_dataset, val_dataset, **train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.3100 | Test Loss: 0.3261\n"
     ]
    }
   ],
   "source": [
    "# Testing performance on Test Dataset\n",
    "test_acc, test_loss = evaluate(mlp_model, test_dataset, device=device)\n",
    "print(f\"Test Accuracy: {test_acc*100:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5.2.1.2) Hyperparameter Tuning with Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5.2.2) CNN on Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is on device: cuda\n",
      "\n",
      "Number of Iterations Per Epoch: 420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100/4200 | loss = 0.3293\n",
      "Iteration: 200/4200 | loss = 0.3878\n",
      "Iteration: 300/4200 | loss = 0.3389\n",
      "Iteration: 400/4200 | loss = 0.3546\n",
      "Epoch: 1 | Train Accuracy: 95.105 | Val Accuracy: 93.767|  Train loss: 0.1590 | Val loss: 0.2003\n",
      "\n",
      "Iteration: 500/4200 | loss = 0.3239\n",
      "Iteration: 600/4200 | loss = 0.2299\n",
      "Iteration: 700/4200 | loss = 0.1933\n",
      "Iteration: 800/4200 | loss = 0.3872\n",
      "Epoch: 2 | Train Accuracy: 95.840 | Val Accuracy: 94.228|  Train loss: 0.1342 | Val loss: 0.1933\n",
      "\n",
      "Iteration: 900/4200 | loss = 0.2999\n",
      "Iteration: 1000/4200 | loss = 0.1070\n",
      "Iteration: 1100/4200 | loss = 0.2478\n",
      "Iteration: 1200/4200 | loss = 0.1778\n",
      "Epoch: 3 | Train Accuracy: 95.967 | Val Accuracy: 94.250|  Train loss: 0.1280 | Val loss: 0.1990\n",
      "\n",
      "Iteration: 1300/4200 | loss = 0.1250\n",
      "Iteration: 1400/4200 | loss = 0.0926\n",
      "Iteration: 1500/4200 | loss = 0.2651\n",
      "Iteration: 1600/4200 | loss = 0.3944\n",
      "Epoch: 4 | Train Accuracy: 96.245 | Val Accuracy: 94.061|  Train loss: 0.1167 | Val loss: 0.2157\n",
      "\n",
      "Iteration: 1700/4200 | loss = 0.1347\n",
      "Iteration: 1800/4200 | loss = 0.1903\n",
      "Iteration: 1900/4200 | loss = 0.1913\n",
      "Iteration: 2000/4200 | loss = 0.2103\n",
      "Iteration: 2100/4200 | loss = 0.3702\n",
      "Epoch: 5 | Train Accuracy: 95.824 | Val Accuracy: 93.372|  Train loss: 0.1251 | Val loss: 0.2229\n",
      "\n",
      "Iteration: 2200/4200 | loss = 0.2352\n",
      "Iteration: 2300/4200 | loss = 0.2289\n",
      "Iteration: 2400/4200 | loss = 0.2978\n",
      "Iteration: 2500/4200 | loss = 0.3578\n",
      "Epoch: 6 | Train Accuracy: 96.271 | Val Accuracy: 93.800|  Train loss: 0.1145 | Val loss: 0.2265\n",
      "\n",
      "Iteration: 2600/4200 | loss = 0.1305\n",
      "Iteration: 2700/4200 | loss = 0.0358\n",
      "Iteration: 2800/4200 | loss = 0.3504\n",
      "Iteration: 2900/4200 | loss = 0.1830\n",
      "Epoch: 7 | Train Accuracy: 96.664 | Val Accuracy: 93.950|  Train loss: 0.1038 | Val loss: 0.2155\n",
      "\n",
      "Iteration: 3000/4200 | loss = 0.2132\n",
      "Iteration: 3100/4200 | loss = 0.1671\n",
      "Iteration: 3200/4200 | loss = 0.0974\n",
      "Iteration: 3300/4200 | loss = 0.2034\n",
      "Epoch: 8 | Train Accuracy: 96.731 | Val Accuracy: 93.978|  Train loss: 0.1150 | Val loss: 0.2696\n",
      "\n",
      "Iteration: 3400/4200 | loss = 0.0570\n",
      "Iteration: 3500/4200 | loss = 0.0972\n",
      "Iteration: 3600/4200 | loss = 0.2312\n",
      "Iteration: 3700/4200 | loss = 0.1092\n",
      "Epoch: 9 | Train Accuracy: 97.181 | Val Accuracy: 94.117|  Train loss: 0.0892 | Val loss: 0.2537\n",
      "\n",
      "Iteration: 3800/4200 | loss = 0.1695\n",
      "Iteration: 3900/4200 | loss = 0.3083\n",
      "Iteration: 4000/4200 | loss = 0.2019\n",
      "Iteration: 4100/4200 | loss = 0.1499\n",
      "Iteration: 4200/4200 | loss = 0.3176\n",
      "Epoch: 10 | Train Accuracy: 97.093 | Val Accuracy: 94.211|  Train loss: 0.0883 | Val loss: 0.2396\n",
      "\n",
      "\n",
      "BEST VAL ACCURACY : 94.2500 | Best Epoch: 3 | Val loss: 0.1990\n"
     ]
    }
   ],
   "source": [
    "cnn_model = SimpleCNN(kernel_size=5, num_channels=128, stride=1, dropout=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.01)\n",
    "\n",
    "train_config = {\n",
    "    \"device\": device,\n",
    "    \"print_every\": 100\n",
    "}\n",
    "\n",
    "_ = train(cnn_model, criterion, optimizer, train_dataset, val_dataset, **train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.0900 | Test Loss: 0.2333\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate(cnn_model, test_dataset, device=device)\n",
    "print(f\"Test Accuracy: {test_acc*100:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5.3) Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
