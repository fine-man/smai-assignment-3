{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - SMAI - Multilayer Perceptron Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vanshg/play/IIIITH/sem5/smai/assignments/assignment-3-fine-man/datasets\n",
      "/home/vanshg/play/IIIITH/sem5/smai/assignments/assignment-3-fine-man/yaml-files/task-3.3/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# enter the Foldername here:\n",
    "FOLDERNAME = \"/home/richard/play/IIITH/sem5/smai/assignments/assignment-3-fine-man\"\n",
    "\n",
    "if FOLDERNAME is None or not os.path.exists(FOLDERNAME):\n",
    "    FOLDERNAME = os.getcwd()\n",
    "\n",
    "PATHNAME = f\"{FOLDERNAME}\"\n",
    "sys.path.append(f\"{FOLDERNAME}\")\n",
    "\n",
    "# DATA_FOLDER = os.path.join(FOLDERNAME, \"SMAI-Dataset-release/IIIT-CFW\")\n",
    "DATA_FOLDER = os.path.join(FOLDERNAME, \"datasets\")\n",
    "YAML_FOLDER = os.path.join(FOLDERNAME, \"yaml-files/task-3.3/\")\n",
    "print(DATA_FOLDER)\n",
    "print(YAML_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some magic so that the notebook will reload external python modules;\n",
    "# see https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "from random import randrange\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import yaml\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *\n",
    "from src.classifiers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.1) - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622    3  222     18.7   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622    3  222     18.7   \n",
       "6  0.08829  12.5   7.87   NaN  0.524  6.012   66.6  5.5605    5  311     15.2   \n",
       "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505    5  311     15.2   \n",
       "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821    5  311     15.2   \n",
       "9  0.17004  12.5   7.87   NaN  0.524  6.004   85.9  6.5921    5  311     15.2   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90    NaN  36.2  \n",
       "5  394.12   5.21  28.7  \n",
       "6  395.60  12.43  22.9  \n",
       "7  396.90  19.15  27.1  \n",
       "8  386.63  29.93  16.5  \n",
       "9  386.71  17.10  18.9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.read_csv(os.path.join(DATA_FOLDER, \"housingdata.csv\"))\n",
    "print(housing_df.shape)\n",
    "housing_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_housing_dataset(housing_df):\n",
    "    for column_name in housing_df.columns:\n",
    "        # Calculate the median of the correct column\n",
    "        median_value = housing_df[column_name].median()\n",
    "\n",
    "        # Fill NaN value in the current column with the median\n",
    "        housing_df[column_name].fillna(median_value, inplace=True)\n",
    "    \n",
    "    \n",
    "    return housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "housing_df = preprocess_housing_dataset(housing_df)\n",
    "print(housing_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.1) - Describing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.479140</td>\n",
       "      <td>10.768775</td>\n",
       "      <td>11.028893</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.845850</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.664625</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.570832</td>\n",
       "      <td>23.025124</td>\n",
       "      <td>6.704679</td>\n",
       "      <td>0.250605</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>27.486962</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.017219</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.083235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.925000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.253715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.808720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>93.575000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.479140   10.768775   11.028893    0.067194    0.554695    6.284634   \n",
       "std      8.570832   23.025124    6.704679    0.250605    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.083235    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.253715    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      2.808720    0.000000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.845850    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     27.486962    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.925000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     76.800000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     93.575000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.664625   22.532806  \n",
       "std      7.017219    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      7.230000   17.025000  \n",
       "50%     11.430000   21.200000  \n",
       "75%     16.570000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.2) - Plotting Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, bin_edges = np.histogram(housing_df['MEDV'].to_numpy(), bins=10)\n",
    "\n",
    "plt.hist(housing_df[\"MEDV\"].to_numpy(), bins=50)\n",
    "plt.title(\"Distribution of housing prices\")\n",
    "plt.xlabel(\"House Prices\")\n",
    "plt.ylabel(\"Distrbution of Prices\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.3) Splitting Data into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "X = housing_df.iloc[:, :-1].to_numpy()\n",
    "y = housing_df.iloc[:, -1].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Data: (354, 13), (354,)\n",
      "Size of Validation Data: (51, 13), (51,)\n",
      "Size of Testing Data: (101, 13), (101,)\n"
     ]
    }
   ],
   "source": [
    "# Train, Val, Test split\n",
    "# 70-10-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Size of Training Data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Size of Validation Data: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Size of Testing Data: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.4) Standerizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizing the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Data: (354, 13), (354,)\n",
      "Size of Validation Data: (51, 13), (51,)\n",
      "Size of Testing Data: (101, 13), (101,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of Training Data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Size of Validation Data: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Size of Testing Data: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.3) Hyper-parameter Tuning using Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out sweep using a single config\n",
    "config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"sgd-grid-search\",\n",
    "    \"parameters\": {\n",
    "        \"criterion\": \"MSE\",\n",
    "        \"model\": {\n",
    "            \"input_dim\" : 13,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_dims1\": 10,\n",
    "            \"num_classes\": 1,\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"update_rule\" : \"adam\",\n",
    "            \"learning_rate\": 1e-2,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"update_type\": \"minibatch\",\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 1000,\n",
    "            \"print_every\": 1000,\n",
    "            \"calc_accuracy\": False,\n",
    "            \"log_wandb\": True,\n",
    "            # \"verbose\": True\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
      "Epoch: 1 |  Train loss: 401.2573 | Val loss: 465.4018\n",
      "\n",
      "Epoch: 2 |  Train loss: 279.2186 | Val loss: 325.2338\n",
      "\n",
      "Epoch: 3 |  Train loss: 180.7680 | Val loss: 214.9920\n",
      "\n",
      "Epoch: 4 |  Train loss: 113.1686 | Val loss: 140.3889\n",
      "\n",
      "Epoch: 5 |  Train loss: 82.6394 | Val loss: 106.3871\n",
      "\n",
      "Epoch: 6 |  Train loss: 66.5285 | Val loss: 93.2593\n",
      "\n",
      "Epoch: 7 |  Train loss: 56.4294 | Val loss: 87.9818\n",
      "\n",
      "Epoch: 8 |  Train loss: 51.1229 | Val loss: 85.5507\n",
      "\n",
      "Epoch: 9 |  Train loss: 46.7816 | Val loss: 80.0999\n",
      "\n",
      "Epoch: 10 |  Train loss: 43.1420 | Val loss: 73.4752\n",
      "\n",
      "Epoch: 11 |  Train loss: 39.5588 | Val loss: 66.6714\n",
      "\n",
      "Epoch: 12 |  Train loss: 36.6973 | Val loss: 63.0194\n",
      "\n",
      "Epoch: 13 |  Train loss: 34.0041 | Val loss: 57.3562\n",
      "\n",
      "Epoch: 14 |  Train loss: 31.3299 | Val loss: 50.9019\n",
      "\n",
      "Epoch: 15 |  Train loss: 29.0564 | Val loss: 45.6673\n",
      "\n",
      "Epoch: 16 |  Train loss: 27.4545 | Val loss: 41.0590\n",
      "\n",
      "Epoch: 17 |  Train loss: 25.4219 | Val loss: 39.1285\n",
      "\n",
      "Epoch: 18 |  Train loss: 24.1331 | Val loss: 36.2712\n",
      "\n",
      "Epoch: 19 |  Train loss: 23.1974 | Val loss: 34.1012\n",
      "\n",
      "Epoch: 20 |  Train loss: 22.4129 | Val loss: 34.5037\n",
      "\n",
      "Epoch: 21 |  Train loss: 21.4528 | Val loss: 33.4467\n",
      "\n",
      "Epoch: 22 |  Train loss: 20.6374 | Val loss: 31.3767\n",
      "\n",
      "Epoch: 23 |  Train loss: 19.8334 | Val loss: 30.0558\n",
      "\n",
      "Epoch: 24 |  Train loss: 19.2551 | Val loss: 29.6034\n",
      "\n",
      "Epoch: 25 |  Train loss: 18.6863 | Val loss: 28.6735\n",
      "\n",
      "Epoch: 26 |  Train loss: 18.2128 | Val loss: 27.5008\n",
      "\n",
      "Epoch: 27 |  Train loss: 17.6812 | Val loss: 27.9568\n",
      "\n",
      "Epoch: 28 |  Train loss: 17.3931 | Val loss: 27.3160\n",
      "\n",
      "Epoch: 29 |  Train loss: 17.0546 | Val loss: 26.2689\n",
      "\n",
      "Epoch: 30 |  Train loss: 16.9141 | Val loss: 25.3148\n",
      "\n",
      "Epoch: 31 |  Train loss: 16.4109 | Val loss: 25.4048\n",
      "\n",
      "Epoch: 32 |  Train loss: 16.1856 | Val loss: 25.2576\n",
      "\n",
      "Epoch: 33 |  Train loss: 15.6782 | Val loss: 24.9404\n",
      "\n",
      "Epoch: 34 |  Train loss: 15.2866 | Val loss: 24.9253\n",
      "\n",
      "Epoch: 35 |  Train loss: 15.0157 | Val loss: 24.9905\n",
      "\n",
      "Epoch: 36 |  Train loss: 15.4142 | Val loss: 23.7432\n",
      "\n",
      "Epoch: 37 |  Train loss: 14.6590 | Val loss: 22.1693\n",
      "\n",
      "Epoch: 38 |  Train loss: 14.3056 | Val loss: 22.8585\n",
      "\n",
      "Epoch: 39 |  Train loss: 14.2939 | Val loss: 23.7937\n",
      "\n",
      "Epoch: 40 |  Train loss: 13.6723 | Val loss: 22.3179\n",
      "\n",
      "Epoch: 41 |  Train loss: 13.4983 | Val loss: 21.5651\n",
      "\n",
      "Epoch: 42 |  Train loss: 13.7536 | Val loss: 23.6045\n",
      "\n",
      "Epoch: 43 |  Train loss: 13.3259 | Val loss: 23.1212\n",
      "\n",
      "Epoch: 44 |  Train loss: 13.3201 | Val loss: 23.2521\n",
      "\n",
      "Epoch: 45 |  Train loss: 13.0846 | Val loss: 22.8282\n",
      "\n",
      "Epoch: 46 |  Train loss: 12.7473 | Val loss: 22.5828\n",
      "\n",
      "Epoch: 47 |  Train loss: 12.4748 | Val loss: 21.7456\n",
      "\n",
      "Epoch: 48 |  Train loss: 12.7211 | Val loss: 22.4812\n",
      "\n",
      "Epoch: 49 |  Train loss: 12.4549 | Val loss: 22.5909\n",
      "\n",
      "Epoch: 50 |  Train loss: 12.0415 | Val loss: 21.5095\n",
      "\n",
      "Epoch: 51 |  Train loss: 11.8825 | Val loss: 21.6178\n",
      "\n",
      "Epoch: 52 |  Train loss: 11.7603 | Val loss: 20.7548\n",
      "\n",
      "Epoch: 53 |  Train loss: 11.5890 | Val loss: 21.1557\n",
      "\n",
      "Epoch: 54 |  Train loss: 11.7497 | Val loss: 21.2901\n",
      "\n",
      "Epoch: 55 |  Train loss: 11.4373 | Val loss: 20.1030\n",
      "\n",
      "Epoch: 56 |  Train loss: 11.5599 | Val loss: 19.4386\n",
      "\n",
      "Epoch: 57 |  Train loss: 11.1761 | Val loss: 20.8145\n",
      "\n",
      "Epoch: 58 |  Train loss: 11.1805 | Val loss: 20.5619\n",
      "\n",
      "Epoch: 59 |  Train loss: 11.0138 | Val loss: 20.7033\n",
      "\n",
      "Epoch: 60 |  Train loss: 10.9458 | Val loss: 20.2156\n",
      "\n",
      "Epoch: 61 |  Train loss: 10.9728 | Val loss: 20.2091\n",
      "\n",
      "Epoch: 62 |  Train loss: 10.9447 | Val loss: 20.3671\n",
      "\n",
      "Epoch: 63 |  Train loss: 10.6303 | Val loss: 19.7549\n",
      "\n",
      "Epoch: 64 |  Train loss: 10.5458 | Val loss: 19.9168\n",
      "\n",
      "Epoch: 65 |  Train loss: 10.4227 | Val loss: 19.1651\n",
      "\n",
      "Epoch: 66 |  Train loss: 10.2992 | Val loss: 19.4324\n",
      "\n",
      "Epoch: 67 |  Train loss: 10.2570 | Val loss: 18.9543\n",
      "\n",
      "Epoch: 68 |  Train loss: 10.3979 | Val loss: 20.3602\n",
      "\n",
      "Epoch: 69 |  Train loss: 10.4684 | Val loss: 18.6957\n",
      "\n",
      "Epoch: 70 |  Train loss: 10.3340 | Val loss: 18.7037\n",
      "\n",
      "Epoch: 71 |  Train loss: 10.1919 | Val loss: 20.0222\n",
      "\n",
      "Epoch: 72 |  Train loss: 9.9435 | Val loss: 18.8529\n",
      "\n",
      "Epoch: 73 |  Train loss: 9.8291 | Val loss: 19.3588\n",
      "\n",
      "Epoch: 74 |  Train loss: 10.0430 | Val loss: 20.5939\n",
      "\n",
      "Epoch: 75 |  Train loss: 10.4176 | Val loss: 21.9363\n",
      "\n",
      "Epoch: 76 |  Train loss: 10.0688 | Val loss: 19.5323\n",
      "\n",
      "Epoch: 77 |  Train loss: 9.8163 | Val loss: 19.1698\n",
      "\n",
      "Epoch: 78 |  Train loss: 9.8175 | Val loss: 20.7485\n",
      "\n",
      "Epoch: 79 |  Train loss: 9.7808 | Val loss: 21.2806\n",
      "\n",
      "Epoch: 80 |  Train loss: 9.6243 | Val loss: 19.3014\n",
      "\n",
      "Epoch: 81 |  Train loss: 9.4257 | Val loss: 19.7685\n",
      "\n",
      "Epoch: 82 |  Train loss: 9.3316 | Val loss: 19.8756\n",
      "\n",
      "Epoch: 83 |  Train loss: 9.2794 | Val loss: 18.8700\n",
      "\n",
      "Iteration: 1000/12000 | loss = 11.4950\n",
      "Epoch: 84 |  Train loss: 9.1903 | Val loss: 19.3282\n",
      "\n",
      "Epoch: 85 |  Train loss: 9.4303 | Val loss: 20.8750\n",
      "\n",
      "Epoch: 86 |  Train loss: 9.6027 | Val loss: 18.5338\n",
      "\n",
      "Epoch: 87 |  Train loss: 9.3854 | Val loss: 19.6508\n",
      "\n",
      "Epoch: 88 |  Train loss: 9.7353 | Val loss: 22.5520\n",
      "\n",
      "Epoch: 89 |  Train loss: 9.3621 | Val loss: 20.3865\n",
      "\n",
      "Epoch: 90 |  Train loss: 9.5261 | Val loss: 20.9858\n",
      "\n",
      "Epoch: 91 |  Train loss: 9.3785 | Val loss: 20.6376\n",
      "\n",
      "Epoch: 92 |  Train loss: 9.2455 | Val loss: 20.5977\n",
      "\n",
      "Epoch: 93 |  Train loss: 9.0587 | Val loss: 19.4531\n",
      "\n",
      "Epoch: 94 |  Train loss: 8.9677 | Val loss: 19.3265\n",
      "\n",
      "Epoch: 95 |  Train loss: 8.9028 | Val loss: 19.9325\n",
      "\n",
      "Epoch: 96 |  Train loss: 9.0612 | Val loss: 18.9416\n",
      "\n",
      "Epoch: 97 |  Train loss: 8.8484 | Val loss: 19.6993\n",
      "\n",
      "Epoch: 98 |  Train loss: 8.8153 | Val loss: 19.2311\n",
      "\n",
      "Epoch: 99 |  Train loss: 8.7485 | Val loss: 19.7749\n",
      "\n",
      "Epoch: 100 |  Train loss: 8.8575 | Val loss: 19.6836\n",
      "\n",
      "Epoch: 101 |  Train loss: 8.8262 | Val loss: 20.5116\n",
      "\n",
      "Epoch: 102 |  Train loss: 8.6858 | Val loss: 19.8725\n",
      "\n",
      "Epoch: 103 |  Train loss: 8.8313 | Val loss: 18.8470\n",
      "\n",
      "Epoch: 104 |  Train loss: 8.6961 | Val loss: 19.7379\n",
      "\n",
      "Epoch: 105 |  Train loss: 8.6514 | Val loss: 20.1241\n",
      "\n",
      "Epoch: 106 |  Train loss: 8.7388 | Val loss: 18.9448\n",
      "\n",
      "Epoch: 107 |  Train loss: 8.6844 | Val loss: 19.8845\n",
      "\n",
      "Epoch: 108 |  Train loss: 8.6469 | Val loss: 19.8162\n",
      "\n",
      "Epoch: 109 |  Train loss: 8.6236 | Val loss: 19.2205\n",
      "\n",
      "Epoch: 110 |  Train loss: 8.6032 | Val loss: 20.3260\n",
      "\n",
      "Epoch: 111 |  Train loss: 8.9922 | Val loss: 19.8759\n",
      "\n",
      "Epoch: 112 |  Train loss: 8.8289 | Val loss: 18.3080\n",
      "\n",
      "Epoch: 113 |  Train loss: 8.6543 | Val loss: 18.8618\n",
      "\n",
      "Epoch: 114 |  Train loss: 9.0044 | Val loss: 18.9849\n",
      "\n",
      "Epoch: 115 |  Train loss: 8.4912 | Val loss: 19.9743\n",
      "\n",
      "Epoch: 116 |  Train loss: 8.7773 | Val loss: 19.3289\n",
      "\n",
      "Epoch: 117 |  Train loss: 8.5645 | Val loss: 20.1206\n",
      "\n",
      "Epoch: 118 |  Train loss: 8.3403 | Val loss: 19.4707\n",
      "\n",
      "Epoch: 119 |  Train loss: 8.4077 | Val loss: 19.0515\n",
      "\n",
      "Epoch: 120 |  Train loss: 8.3311 | Val loss: 19.9441\n",
      "\n",
      "Epoch: 121 |  Train loss: 8.3603 | Val loss: 19.3032\n",
      "\n",
      "Epoch: 122 |  Train loss: 8.3991 | Val loss: 20.4898\n",
      "\n",
      "Epoch: 123 |  Train loss: 8.3546 | Val loss: 19.2993\n",
      "\n",
      "Epoch: 124 |  Train loss: 8.3856 | Val loss: 19.0035\n",
      "\n",
      "Epoch: 125 |  Train loss: 8.2809 | Val loss: 18.8897\n",
      "\n",
      "Epoch: 126 |  Train loss: 8.3162 | Val loss: 19.7540\n",
      "\n",
      "Epoch: 127 |  Train loss: 8.2090 | Val loss: 20.1554\n",
      "\n",
      "Epoch: 128 |  Train loss: 8.1432 | Val loss: 19.1033\n",
      "\n",
      "Epoch: 129 |  Train loss: 8.1138 | Val loss: 19.6982\n",
      "\n",
      "Epoch: 130 |  Train loss: 8.1495 | Val loss: 19.7670\n",
      "\n",
      "Epoch: 131 |  Train loss: 8.2544 | Val loss: 19.0690\n",
      "\n",
      "Epoch: 132 |  Train loss: 8.2519 | Val loss: 20.3079\n",
      "\n",
      "Epoch: 133 |  Train loss: 8.1083 | Val loss: 18.8375\n",
      "\n",
      "Epoch: 134 |  Train loss: 7.9955 | Val loss: 19.6328\n",
      "\n",
      "Epoch: 135 |  Train loss: 7.9804 | Val loss: 19.2877\n",
      "\n",
      "Epoch: 136 |  Train loss: 8.0107 | Val loss: 19.6711\n",
      "\n",
      "Epoch: 137 |  Train loss: 7.9095 | Val loss: 19.4869\n",
      "\n",
      "Epoch: 138 |  Train loss: 7.9790 | Val loss: 19.5444\n",
      "\n",
      "Epoch: 139 |  Train loss: 7.9751 | Val loss: 20.6098\n",
      "\n",
      "Epoch: 140 |  Train loss: 7.9042 | Val loss: 19.5143\n",
      "\n",
      "Epoch: 141 |  Train loss: 8.0288 | Val loss: 19.2398\n",
      "\n",
      "Epoch: 142 |  Train loss: 8.1434 | Val loss: 19.8536\n",
      "\n",
      "Epoch: 143 |  Train loss: 8.0822 | Val loss: 21.4225\n",
      "\n",
      "Epoch: 144 |  Train loss: 7.9629 | Val loss: 18.7594\n",
      "\n",
      "Epoch: 145 |  Train loss: 7.8625 | Val loss: 19.4336\n",
      "\n",
      "Epoch: 146 |  Train loss: 8.2410 | Val loss: 19.0704\n",
      "\n",
      "Epoch: 147 |  Train loss: 8.1877 | Val loss: 19.8565\n",
      "\n",
      "Epoch: 148 |  Train loss: 7.8034 | Val loss: 19.5420\n",
      "\n",
      "Epoch: 149 |  Train loss: 7.9585 | Val loss: 21.0272\n",
      "\n",
      "Epoch: 150 |  Train loss: 7.7943 | Val loss: 19.3873\n",
      "\n",
      "Epoch: 151 |  Train loss: 7.7322 | Val loss: 19.3476\n",
      "\n",
      "Epoch: 152 |  Train loss: 7.7574 | Val loss: 18.5361\n",
      "\n",
      "Epoch: 153 |  Train loss: 7.6814 | Val loss: 19.9313\n",
      "\n",
      "Epoch: 154 |  Train loss: 7.6648 | Val loss: 19.1007\n",
      "\n",
      "Epoch: 155 |  Train loss: 7.8196 | Val loss: 20.9986\n",
      "\n",
      "Epoch: 156 |  Train loss: 7.7577 | Val loss: 20.3228\n",
      "\n",
      "Epoch: 157 |  Train loss: 7.6155 | Val loss: 19.4915\n",
      "\n",
      "Epoch: 158 |  Train loss: 7.6543 | Val loss: 20.2253\n",
      "\n",
      "Epoch: 159 |  Train loss: 7.8255 | Val loss: 21.1693\n",
      "\n",
      "Epoch: 160 |  Train loss: 7.7123 | Val loss: 19.7699\n",
      "\n",
      "Epoch: 161 |  Train loss: 8.0282 | Val loss: 18.6056\n",
      "\n",
      "Epoch: 162 |  Train loss: 7.7078 | Val loss: 20.2431\n",
      "\n",
      "Epoch: 163 |  Train loss: 7.5622 | Val loss: 19.4649\n",
      "\n",
      "Epoch: 164 |  Train loss: 7.5535 | Val loss: 19.7900\n",
      "\n",
      "Epoch: 165 |  Train loss: 7.5800 | Val loss: 20.2637\n",
      "\n",
      "Epoch: 166 |  Train loss: 7.6017 | Val loss: 19.0528\n",
      "\n",
      "Iteration: 2000/12000 | loss = 5.2288\n",
      "Epoch: 167 |  Train loss: 7.5117 | Val loss: 20.0807\n",
      "\n",
      "Epoch: 168 |  Train loss: 7.5923 | Val loss: 20.1727\n",
      "\n",
      "Epoch: 169 |  Train loss: 7.6320 | Val loss: 20.2064\n",
      "\n",
      "Epoch: 170 |  Train loss: 7.6349 | Val loss: 20.0877\n",
      "\n",
      "Epoch: 171 |  Train loss: 7.5797 | Val loss: 19.2293\n",
      "\n",
      "Epoch: 172 |  Train loss: 7.5448 | Val loss: 21.0130\n",
      "\n",
      "Epoch: 173 |  Train loss: 7.4426 | Val loss: 19.7203\n",
      "\n",
      "Epoch: 174 |  Train loss: 7.5117 | Val loss: 19.7838\n",
      "\n",
      "Epoch: 175 |  Train loss: 7.5226 | Val loss: 21.2291\n",
      "\n",
      "Epoch: 176 |  Train loss: 7.4783 | Val loss: 19.8154\n",
      "\n",
      "Epoch: 177 |  Train loss: 7.4852 | Val loss: 20.9663\n",
      "\n",
      "Epoch: 178 |  Train loss: 7.4143 | Val loss: 20.5987\n",
      "\n",
      "Epoch: 179 |  Train loss: 7.3905 | Val loss: 20.2483\n",
      "\n",
      "Epoch: 180 |  Train loss: 7.4798 | Val loss: 20.8188\n",
      "\n",
      "Epoch: 181 |  Train loss: 7.6224 | Val loss: 21.0281\n",
      "\n",
      "Epoch: 182 |  Train loss: 7.4031 | Val loss: 19.6173\n",
      "\n",
      "Epoch: 183 |  Train loss: 7.5480 | Val loss: 21.0004\n",
      "\n",
      "Epoch: 184 |  Train loss: 7.4546 | Val loss: 18.5338\n",
      "\n",
      "Epoch: 185 |  Train loss: 7.4228 | Val loss: 20.6718\n",
      "\n",
      "Epoch: 186 |  Train loss: 7.3406 | Val loss: 19.0094\n",
      "\n",
      "Epoch: 187 |  Train loss: 7.2956 | Val loss: 20.1269\n",
      "\n",
      "Epoch: 188 |  Train loss: 7.3857 | Val loss: 21.1959\n",
      "\n",
      "Epoch: 189 |  Train loss: 7.8059 | Val loss: 21.7724\n",
      "\n",
      "Epoch: 190 |  Train loss: 7.4924 | Val loss: 20.2106\n",
      "\n",
      "Epoch: 191 |  Train loss: 7.4102 | Val loss: 20.8779\n",
      "\n",
      "Epoch: 192 |  Train loss: 7.2771 | Val loss: 19.9976\n",
      "\n",
      "Epoch: 193 |  Train loss: 7.2412 | Val loss: 19.8758\n",
      "\n",
      "Epoch: 194 |  Train loss: 7.5192 | Val loss: 19.5698\n",
      "\n",
      "Epoch: 195 |  Train loss: 7.2942 | Val loss: 20.2364\n",
      "\n",
      "Epoch: 196 |  Train loss: 7.2809 | Val loss: 19.6456\n",
      "\n",
      "Epoch: 197 |  Train loss: 7.2963 | Val loss: 20.4404\n",
      "\n",
      "Epoch: 198 |  Train loss: 7.2442 | Val loss: 19.6134\n",
      "\n",
      "Epoch: 199 |  Train loss: 7.2376 | Val loss: 19.8562\n",
      "\n",
      "Epoch: 200 |  Train loss: 7.2616 | Val loss: 20.7047\n",
      "\n",
      "Epoch: 201 |  Train loss: 7.6077 | Val loss: 19.3095\n",
      "\n",
      "Epoch: 202 |  Train loss: 7.6342 | Val loss: 21.8632\n",
      "\n",
      "Epoch: 203 |  Train loss: 7.3691 | Val loss: 19.7484\n",
      "\n",
      "Epoch: 204 |  Train loss: 7.1684 | Val loss: 20.7272\n",
      "\n",
      "Epoch: 205 |  Train loss: 7.8089 | Val loss: 19.9837\n",
      "\n",
      "Epoch: 206 |  Train loss: 7.3742 | Val loss: 19.5734\n",
      "\n",
      "Epoch: 207 |  Train loss: 7.2432 | Val loss: 19.3995\n",
      "\n",
      "Epoch: 208 |  Train loss: 7.0700 | Val loss: 19.6816\n",
      "\n",
      "Epoch: 209 |  Train loss: 7.1457 | Val loss: 20.1769\n",
      "\n",
      "Epoch: 210 |  Train loss: 7.0763 | Val loss: 20.3385\n",
      "\n",
      "Epoch: 211 |  Train loss: 6.9895 | Val loss: 20.2657\n",
      "\n",
      "Epoch: 212 |  Train loss: 7.0248 | Val loss: 19.8737\n",
      "\n",
      "Epoch: 213 |  Train loss: 7.0940 | Val loss: 20.5289\n",
      "\n",
      "Epoch: 214 |  Train loss: 7.4733 | Val loss: 21.1154\n",
      "\n",
      "Epoch: 215 |  Train loss: 7.2598 | Val loss: 20.0345\n",
      "\n",
      "Epoch: 216 |  Train loss: 7.1373 | Val loss: 20.2480\n",
      "\n",
      "Epoch: 217 |  Train loss: 7.3095 | Val loss: 19.7028\n",
      "\n",
      "Epoch: 218 |  Train loss: 7.3570 | Val loss: 21.0995\n",
      "\n",
      "Epoch: 219 |  Train loss: 7.0788 | Val loss: 20.0774\n",
      "\n",
      "Epoch: 220 |  Train loss: 6.9664 | Val loss: 19.9478\n",
      "\n",
      "Epoch: 221 |  Train loss: 7.0261 | Val loss: 20.2136\n",
      "\n",
      "Epoch: 222 |  Train loss: 6.9518 | Val loss: 20.2500\n",
      "\n",
      "Epoch: 223 |  Train loss: 6.9112 | Val loss: 20.3312\n",
      "\n",
      "Epoch: 224 |  Train loss: 6.8475 | Val loss: 20.1308\n",
      "\n",
      "Epoch: 225 |  Train loss: 6.9222 | Val loss: 19.8145\n",
      "\n",
      "Epoch: 226 |  Train loss: 7.1311 | Val loss: 20.4800\n",
      "\n",
      "Epoch: 227 |  Train loss: 7.0600 | Val loss: 19.9512\n",
      "\n",
      "Epoch: 228 |  Train loss: 6.8554 | Val loss: 19.7206\n",
      "\n",
      "Epoch: 229 |  Train loss: 6.8786 | Val loss: 20.3913\n",
      "\n",
      "Epoch: 230 |  Train loss: 6.8343 | Val loss: 20.7548\n",
      "\n",
      "Epoch: 231 |  Train loss: 6.9350 | Val loss: 21.2399\n",
      "\n",
      "Epoch: 232 |  Train loss: 6.8814 | Val loss: 20.5249\n",
      "\n",
      "Epoch: 233 |  Train loss: 7.2520 | Val loss: 22.1207\n",
      "\n",
      "Epoch: 234 |  Train loss: 7.3222 | Val loss: 21.1259\n",
      "\n",
      "Epoch: 235 |  Train loss: 7.0370 | Val loss: 19.8795\n",
      "\n",
      "Epoch: 236 |  Train loss: 6.8547 | Val loss: 19.3944\n",
      "\n",
      "Epoch: 237 |  Train loss: 6.8238 | Val loss: 19.0277\n",
      "\n",
      "Epoch: 238 |  Train loss: 7.0909 | Val loss: 18.9675\n",
      "\n",
      "Epoch: 239 |  Train loss: 6.9981 | Val loss: 18.7604\n",
      "\n",
      "Epoch: 240 |  Train loss: 6.9605 | Val loss: 18.5785\n",
      "\n",
      "Epoch: 241 |  Train loss: 6.8483 | Val loss: 19.7682\n",
      "\n",
      "Epoch: 242 |  Train loss: 6.7399 | Val loss: 19.1109\n",
      "\n",
      "Epoch: 243 |  Train loss: 6.7377 | Val loss: 20.0848\n",
      "\n",
      "Epoch: 244 |  Train loss: 6.6806 | Val loss: 19.1724\n",
      "\n",
      "Epoch: 245 |  Train loss: 6.7765 | Val loss: 19.7874\n",
      "\n",
      "Epoch: 246 |  Train loss: 6.9744 | Val loss: 20.6245\n",
      "\n",
      "Epoch: 247 |  Train loss: 6.8480 | Val loss: 18.1533\n",
      "\n",
      "Epoch: 248 |  Train loss: 6.8125 | Val loss: 19.7405\n",
      "\n",
      "Epoch: 249 |  Train loss: 6.6843 | Val loss: 18.7840\n",
      "\n",
      "Iteration: 3000/12000 | loss = 4.7302\n",
      "Epoch: 250 |  Train loss: 6.6370 | Val loss: 19.6913\n",
      "\n",
      "Epoch: 251 |  Train loss: 6.7156 | Val loss: 21.1320\n",
      "\n",
      "Epoch: 252 |  Train loss: 6.6126 | Val loss: 19.0295\n",
      "\n",
      "Epoch: 253 |  Train loss: 6.7459 | Val loss: 20.0123\n",
      "\n",
      "Epoch: 254 |  Train loss: 7.3973 | Val loss: 19.6145\n",
      "\n",
      "Epoch: 255 |  Train loss: 6.9465 | Val loss: 20.9003\n",
      "\n",
      "Epoch: 256 |  Train loss: 6.6950 | Val loss: 19.6243\n",
      "\n",
      "Epoch: 257 |  Train loss: 6.7098 | Val loss: 19.1695\n",
      "\n",
      "Epoch: 258 |  Train loss: 6.7033 | Val loss: 19.2053\n",
      "\n",
      "Epoch: 259 |  Train loss: 6.6744 | Val loss: 20.6709\n",
      "\n",
      "Epoch: 260 |  Train loss: 6.6081 | Val loss: 18.6079\n",
      "\n",
      "Epoch: 261 |  Train loss: 6.6358 | Val loss: 19.1133\n",
      "\n",
      "Epoch: 262 |  Train loss: 6.5257 | Val loss: 18.9333\n",
      "\n",
      "Epoch: 263 |  Train loss: 6.7063 | Val loss: 20.2155\n",
      "\n",
      "Epoch: 264 |  Train loss: 7.1898 | Val loss: 18.4348\n",
      "\n",
      "Epoch: 265 |  Train loss: 6.8596 | Val loss: 18.5281\n",
      "\n",
      "Epoch: 266 |  Train loss: 6.6817 | Val loss: 20.3479\n",
      "\n",
      "Epoch: 267 |  Train loss: 6.6435 | Val loss: 18.8595\n",
      "\n",
      "Epoch: 268 |  Train loss: 6.5367 | Val loss: 18.7692\n",
      "\n",
      "Epoch: 269 |  Train loss: 6.5412 | Val loss: 18.8461\n",
      "\n",
      "Epoch: 270 |  Train loss: 7.1119 | Val loss: 21.7691\n",
      "\n",
      "Epoch: 271 |  Train loss: 7.0232 | Val loss: 22.0218\n",
      "\n",
      "Epoch: 272 |  Train loss: 6.6464 | Val loss: 18.3392\n",
      "\n",
      "Epoch: 273 |  Train loss: 6.5003 | Val loss: 20.4420\n",
      "\n",
      "Epoch: 274 |  Train loss: 6.5218 | Val loss: 19.0531\n",
      "\n",
      "Epoch: 275 |  Train loss: 6.4612 | Val loss: 20.7902\n",
      "\n",
      "Epoch: 276 |  Train loss: 6.4819 | Val loss: 19.7966\n",
      "\n",
      "Epoch: 277 |  Train loss: 6.4692 | Val loss: 19.9260\n",
      "\n",
      "Epoch: 278 |  Train loss: 6.8585 | Val loss: 19.8347\n",
      "\n",
      "Epoch: 279 |  Train loss: 6.6776 | Val loss: 18.3994\n",
      "\n",
      "Epoch: 280 |  Train loss: 6.6150 | Val loss: 19.9040\n",
      "\n",
      "Epoch: 281 |  Train loss: 6.5124 | Val loss: 19.0906\n",
      "\n",
      "Epoch: 282 |  Train loss: 6.4447 | Val loss: 19.6039\n",
      "\n",
      "Epoch: 283 |  Train loss: 6.5388 | Val loss: 19.4030\n",
      "\n",
      "Epoch: 284 |  Train loss: 6.4352 | Val loss: 18.9516\n",
      "\n",
      "Epoch: 285 |  Train loss: 6.5598 | Val loss: 18.0496\n",
      "\n",
      "Epoch: 286 |  Train loss: 6.5237 | Val loss: 19.9184\n",
      "\n",
      "Epoch: 287 |  Train loss: 6.3833 | Val loss: 19.2616\n",
      "\n",
      "Epoch: 288 |  Train loss: 6.5488 | Val loss: 19.9746\n",
      "\n",
      "Epoch: 289 |  Train loss: 6.3664 | Val loss: 19.4969\n",
      "\n",
      "Epoch: 290 |  Train loss: 6.3600 | Val loss: 19.7540\n",
      "\n",
      "Epoch: 291 |  Train loss: 6.4432 | Val loss: 19.9142\n",
      "\n",
      "Epoch: 292 |  Train loss: 6.3557 | Val loss: 19.5333\n",
      "\n",
      "Epoch: 293 |  Train loss: 6.2874 | Val loss: 19.3891\n",
      "\n",
      "Epoch: 294 |  Train loss: 6.4019 | Val loss: 19.8087\n",
      "\n",
      "Epoch: 295 |  Train loss: 6.4683 | Val loss: 18.6032\n",
      "\n",
      "Epoch: 296 |  Train loss: 6.4726 | Val loss: 20.7958\n",
      "\n",
      "Epoch: 297 |  Train loss: 6.4068 | Val loss: 19.6480\n",
      "\n",
      "Epoch: 298 |  Train loss: 6.5234 | Val loss: 20.2756\n",
      "\n",
      "Epoch: 299 |  Train loss: 6.4056 | Val loss: 20.0459\n",
      "\n",
      "Epoch: 300 |  Train loss: 6.4858 | Val loss: 19.9342\n",
      "\n",
      "Epoch: 301 |  Train loss: 6.4646 | Val loss: 19.8913\n",
      "\n",
      "Epoch: 302 |  Train loss: 6.9512 | Val loss: 21.4455\n",
      "\n",
      "Epoch: 303 |  Train loss: 6.6384 | Val loss: 18.6256\n",
      "\n",
      "Epoch: 304 |  Train loss: 6.4068 | Val loss: 20.6449\n",
      "\n",
      "Epoch: 305 |  Train loss: 6.2645 | Val loss: 19.8145\n",
      "\n",
      "Epoch: 306 |  Train loss: 6.3702 | Val loss: 18.9966\n",
      "\n",
      "Epoch: 307 |  Train loss: 6.7016 | Val loss: 22.8829\n",
      "\n",
      "Epoch: 308 |  Train loss: 6.5676 | Val loss: 19.2644\n",
      "\n",
      "Epoch: 309 |  Train loss: 6.4524 | Val loss: 20.0034\n",
      "\n",
      "Epoch: 310 |  Train loss: 6.3011 | Val loss: 20.9831\n",
      "\n",
      "Epoch: 311 |  Train loss: 6.8038 | Val loss: 22.1835\n",
      "\n",
      "Epoch: 312 |  Train loss: 6.4522 | Val loss: 20.1402\n",
      "\n",
      "Epoch: 313 |  Train loss: 6.3749 | Val loss: 21.2102\n",
      "\n",
      "Epoch: 314 |  Train loss: 6.4954 | Val loss: 19.6210\n",
      "\n",
      "Epoch: 315 |  Train loss: 6.2677 | Val loss: 20.3351\n",
      "\n",
      "Epoch: 316 |  Train loss: 6.2350 | Val loss: 19.7380\n",
      "\n",
      "Epoch: 317 |  Train loss: 6.2546 | Val loss: 20.0212\n",
      "\n",
      "Epoch: 318 |  Train loss: 6.2399 | Val loss: 19.8247\n",
      "\n",
      "Epoch: 319 |  Train loss: 6.3958 | Val loss: 19.1266\n",
      "\n",
      "Epoch: 320 |  Train loss: 6.4951 | Val loss: 20.5075\n",
      "\n",
      "Epoch: 321 |  Train loss: 6.6007 | Val loss: 18.6667\n",
      "\n",
      "Epoch: 322 |  Train loss: 6.4012 | Val loss: 20.2082\n",
      "\n",
      "Epoch: 323 |  Train loss: 6.3415 | Val loss: 20.4952\n",
      "\n",
      "Epoch: 324 |  Train loss: 6.3933 | Val loss: 21.8965\n",
      "\n",
      "Epoch: 325 |  Train loss: 6.3930 | Val loss: 20.6767\n",
      "\n",
      "Epoch: 326 |  Train loss: 6.3907 | Val loss: 21.1058\n",
      "\n",
      "Epoch: 327 |  Train loss: 6.4598 | Val loss: 21.2549\n",
      "\n",
      "Epoch: 328 |  Train loss: 6.2520 | Val loss: 20.3685\n",
      "\n",
      "Epoch: 329 |  Train loss: 6.2349 | Val loss: 19.7974\n",
      "\n",
      "Epoch: 330 |  Train loss: 6.1724 | Val loss: 20.4791\n",
      "\n",
      "Epoch: 331 |  Train loss: 6.2359 | Val loss: 19.1907\n",
      "\n",
      "Epoch: 332 |  Train loss: 6.1571 | Val loss: 19.2518\n",
      "\n",
      "Epoch: 333 |  Train loss: 6.1483 | Val loss: 20.4828\n",
      "\n",
      "Iteration: 4000/12000 | loss = 7.6721\n",
      "Epoch: 334 |  Train loss: 6.1520 | Val loss: 18.6193\n",
      "\n",
      "Epoch: 335 |  Train loss: 6.1031 | Val loss: 19.9601\n",
      "\n",
      "Epoch: 336 |  Train loss: 6.1456 | Val loss: 19.9551\n",
      "\n",
      "Epoch: 337 |  Train loss: 6.1956 | Val loss: 20.5761\n",
      "\n",
      "Epoch: 338 |  Train loss: 6.1436 | Val loss: 20.9135\n",
      "\n",
      "Epoch: 339 |  Train loss: 6.2051 | Val loss: 19.9690\n",
      "\n",
      "Epoch: 340 |  Train loss: 6.4323 | Val loss: 19.9147\n",
      "\n",
      "Epoch: 341 |  Train loss: 6.3412 | Val loss: 21.4806\n",
      "\n",
      "Epoch: 342 |  Train loss: 6.0962 | Val loss: 19.1431\n",
      "\n",
      "Epoch: 343 |  Train loss: 6.2161 | Val loss: 21.5285\n",
      "\n",
      "Epoch: 344 |  Train loss: 6.5361 | Val loss: 22.2856\n",
      "\n",
      "Epoch: 345 |  Train loss: 6.2437 | Val loss: 20.0607\n",
      "\n",
      "Epoch: 346 |  Train loss: 6.3305 | Val loss: 20.7681\n",
      "\n",
      "Epoch: 347 |  Train loss: 6.1232 | Val loss: 20.0317\n",
      "\n",
      "Epoch: 348 |  Train loss: 6.0622 | Val loss: 20.1557\n",
      "\n",
      "Epoch: 349 |  Train loss: 6.1090 | Val loss: 20.9753\n",
      "\n",
      "Epoch: 350 |  Train loss: 6.0077 | Val loss: 20.2965\n",
      "\n",
      "Epoch: 351 |  Train loss: 6.0917 | Val loss: 21.0854\n",
      "\n",
      "Epoch: 352 |  Train loss: 6.0853 | Val loss: 19.6663\n",
      "\n",
      "Epoch: 353 |  Train loss: 6.3858 | Val loss: 21.5651\n",
      "\n",
      "Epoch: 354 |  Train loss: 6.3972 | Val loss: 20.9357\n",
      "\n",
      "Epoch: 355 |  Train loss: 6.2862 | Val loss: 21.9332\n",
      "\n",
      "Epoch: 356 |  Train loss: 6.2599 | Val loss: 20.2809\n",
      "\n",
      "Epoch: 357 |  Train loss: 6.2088 | Val loss: 22.8381\n",
      "\n",
      "Epoch: 358 |  Train loss: 6.0513 | Val loss: 20.2191\n",
      "\n",
      "Epoch: 359 |  Train loss: 6.0755 | Val loss: 22.6269\n",
      "\n",
      "Epoch: 360 |  Train loss: 5.9974 | Val loss: 20.8350\n",
      "\n",
      "Epoch: 361 |  Train loss: 5.9928 | Val loss: 21.5056\n",
      "\n",
      "Epoch: 362 |  Train loss: 6.1163 | Val loss: 20.1459\n",
      "\n",
      "Epoch: 363 |  Train loss: 6.0154 | Val loss: 20.7658\n",
      "\n",
      "Epoch: 364 |  Train loss: 6.0466 | Val loss: 19.7255\n",
      "\n",
      "Epoch: 365 |  Train loss: 6.0685 | Val loss: 20.7545\n",
      "\n",
      "Epoch: 366 |  Train loss: 6.0010 | Val loss: 21.6077\n",
      "\n",
      "Epoch: 367 |  Train loss: 6.0203 | Val loss: 21.2710\n",
      "\n",
      "Epoch: 368 |  Train loss: 5.9825 | Val loss: 20.9046\n",
      "\n",
      "Epoch: 369 |  Train loss: 5.9517 | Val loss: 21.0426\n",
      "\n",
      "Epoch: 370 |  Train loss: 6.1292 | Val loss: 19.5565\n",
      "\n",
      "Epoch: 371 |  Train loss: 6.1380 | Val loss: 21.0204\n",
      "\n",
      "Epoch: 372 |  Train loss: 5.9958 | Val loss: 20.0280\n",
      "\n",
      "Epoch: 373 |  Train loss: 6.0977 | Val loss: 20.7211\n",
      "\n",
      "Epoch: 374 |  Train loss: 5.9869 | Val loss: 21.6036\n",
      "\n",
      "Epoch: 375 |  Train loss: 5.9324 | Val loss: 20.4657\n",
      "\n",
      "Epoch: 376 |  Train loss: 6.1654 | Val loss: 22.2124\n",
      "\n",
      "Epoch: 377 |  Train loss: 6.0494 | Val loss: 19.8774\n",
      "\n",
      "Epoch: 378 |  Train loss: 6.0872 | Val loss: 22.5432\n",
      "\n",
      "Epoch: 379 |  Train loss: 5.9470 | Val loss: 20.0309\n",
      "\n",
      "Epoch: 380 |  Train loss: 6.2206 | Val loss: 19.6896\n",
      "\n",
      "Epoch: 381 |  Train loss: 6.0534 | Val loss: 21.4353\n",
      "\n",
      "Epoch: 382 |  Train loss: 5.9291 | Val loss: 20.5657\n",
      "\n",
      "Epoch: 383 |  Train loss: 6.1985 | Val loss: 22.1247\n",
      "\n",
      "Epoch: 384 |  Train loss: 6.1089 | Val loss: 20.6589\n",
      "\n",
      "Epoch: 385 |  Train loss: 5.9915 | Val loss: 21.1819\n",
      "\n",
      "Epoch: 386 |  Train loss: 6.2177 | Val loss: 19.5968\n",
      "\n",
      "Epoch: 387 |  Train loss: 6.1097 | Val loss: 19.9502\n",
      "\n",
      "Epoch: 388 |  Train loss: 5.9811 | Val loss: 21.0746\n",
      "\n",
      "Epoch: 389 |  Train loss: 5.9820 | Val loss: 19.1949\n",
      "\n",
      "Epoch: 390 |  Train loss: 6.2795 | Val loss: 20.6905\n",
      "\n",
      "Epoch: 391 |  Train loss: 6.1620 | Val loss: 19.7470\n",
      "\n",
      "Epoch: 392 |  Train loss: 6.0230 | Val loss: 22.7067\n",
      "\n",
      "Epoch: 393 |  Train loss: 6.1585 | Val loss: 20.6601\n",
      "\n",
      "Epoch: 394 |  Train loss: 6.0179 | Val loss: 20.5720\n",
      "\n",
      "Epoch: 395 |  Train loss: 5.9554 | Val loss: 20.5391\n",
      "\n",
      "Epoch: 396 |  Train loss: 5.9553 | Val loss: 20.9494\n",
      "\n",
      "Epoch: 397 |  Train loss: 6.0010 | Val loss: 21.8744\n",
      "\n",
      "Epoch: 398 |  Train loss: 5.9229 | Val loss: 20.1530\n",
      "\n",
      "Epoch: 399 |  Train loss: 5.8971 | Val loss: 20.3319\n",
      "\n",
      "Epoch: 400 |  Train loss: 5.9308 | Val loss: 20.0640\n",
      "\n",
      "Epoch: 401 |  Train loss: 6.5426 | Val loss: 22.4775\n",
      "\n",
      "Epoch: 402 |  Train loss: 6.0946 | Val loss: 20.8119\n",
      "\n",
      "Epoch: 403 |  Train loss: 6.2191 | Val loss: 19.5062\n",
      "\n",
      "Epoch: 404 |  Train loss: 6.1187 | Val loss: 22.0645\n",
      "\n",
      "Epoch: 405 |  Train loss: 5.9844 | Val loss: 19.5497\n",
      "\n",
      "Epoch: 406 |  Train loss: 6.0331 | Val loss: 22.4584\n",
      "\n",
      "Epoch: 407 |  Train loss: 6.1824 | Val loss: 19.5510\n",
      "\n",
      "Epoch: 408 |  Train loss: 5.8651 | Val loss: 20.5219\n",
      "\n",
      "Epoch: 409 |  Train loss: 5.9012 | Val loss: 20.7145\n",
      "\n",
      "Epoch: 410 |  Train loss: 5.9071 | Val loss: 19.5654\n",
      "\n",
      "Epoch: 411 |  Train loss: 5.8868 | Val loss: 19.6231\n",
      "\n",
      "Epoch: 412 |  Train loss: 6.2086 | Val loss: 19.5668\n",
      "\n",
      "Epoch: 413 |  Train loss: 5.9866 | Val loss: 19.4284\n",
      "\n",
      "Epoch: 414 |  Train loss: 6.1805 | Val loss: 19.3914\n",
      "\n",
      "Epoch: 415 |  Train loss: 6.0039 | Val loss: 22.1522\n",
      "\n",
      "Epoch: 416 |  Train loss: 6.1127 | Val loss: 21.4350\n",
      "\n",
      "Iteration: 5000/12000 | loss = 4.2126\n",
      "Epoch: 417 |  Train loss: 5.8893 | Val loss: 19.6762\n",
      "\n",
      "Epoch: 418 |  Train loss: 6.1410 | Val loss: 22.1751\n",
      "\n",
      "Epoch: 419 |  Train loss: 5.9865 | Val loss: 19.8647\n",
      "\n",
      "Epoch: 420 |  Train loss: 5.9154 | Val loss: 21.1825\n",
      "\n",
      "Epoch: 421 |  Train loss: 5.8598 | Val loss: 20.0820\n",
      "\n",
      "Epoch: 422 |  Train loss: 5.9483 | Val loss: 20.0819\n",
      "\n",
      "Epoch: 423 |  Train loss: 6.0941 | Val loss: 21.4226\n",
      "\n",
      "Epoch: 424 |  Train loss: 5.8748 | Val loss: 20.1964\n",
      "\n",
      "Epoch: 425 |  Train loss: 5.8206 | Val loss: 20.9937\n",
      "\n",
      "Epoch: 426 |  Train loss: 5.8261 | Val loss: 18.9891\n",
      "\n",
      "Epoch: 427 |  Train loss: 5.7983 | Val loss: 21.0645\n",
      "\n",
      "Epoch: 428 |  Train loss: 5.9854 | Val loss: 20.2256\n",
      "\n",
      "Epoch: 429 |  Train loss: 6.0493 | Val loss: 21.1146\n",
      "\n",
      "Epoch: 430 |  Train loss: 5.9385 | Val loss: 19.2104\n",
      "\n",
      "Epoch: 431 |  Train loss: 5.8309 | Val loss: 20.5048\n",
      "\n",
      "Epoch: 432 |  Train loss: 5.8421 | Val loss: 20.2527\n",
      "\n",
      "Epoch: 433 |  Train loss: 6.0781 | Val loss: 18.7882\n",
      "\n",
      "Epoch: 434 |  Train loss: 6.0892 | Val loss: 19.1547\n",
      "\n",
      "Epoch: 435 |  Train loss: 6.0501 | Val loss: 19.3358\n",
      "\n",
      "Epoch: 436 |  Train loss: 6.1712 | Val loss: 21.4222\n",
      "\n",
      "Epoch: 437 |  Train loss: 5.9790 | Val loss: 18.0286\n",
      "\n",
      "Epoch: 438 |  Train loss: 5.9451 | Val loss: 20.0772\n",
      "\n",
      "Epoch: 439 |  Train loss: 5.7982 | Val loss: 19.2009\n",
      "\n",
      "Epoch: 440 |  Train loss: 5.7929 | Val loss: 19.8146\n",
      "\n",
      "Epoch: 441 |  Train loss: 5.8295 | Val loss: 19.1261\n",
      "\n",
      "Epoch: 442 |  Train loss: 5.9588 | Val loss: 20.1511\n",
      "\n",
      "Epoch: 443 |  Train loss: 6.0180 | Val loss: 20.1999\n",
      "\n",
      "Epoch: 444 |  Train loss: 5.8821 | Val loss: 19.1671\n",
      "\n",
      "Epoch: 445 |  Train loss: 6.5002 | Val loss: 23.9792\n",
      "\n",
      "Epoch: 446 |  Train loss: 6.3629 | Val loss: 21.7919\n",
      "\n",
      "Epoch: 447 |  Train loss: 5.8856 | Val loss: 21.4682\n",
      "\n",
      "Epoch: 448 |  Train loss: 5.8861 | Val loss: 19.7883\n",
      "\n",
      "Epoch: 449 |  Train loss: 5.8168 | Val loss: 19.4955\n",
      "\n",
      "Epoch: 450 |  Train loss: 5.8477 | Val loss: 19.8428\n",
      "\n",
      "Epoch: 451 |  Train loss: 5.8974 | Val loss: 19.3827\n",
      "\n",
      "Epoch: 452 |  Train loss: 5.8913 | Val loss: 19.1103\n",
      "\n",
      "Epoch: 453 |  Train loss: 5.9203 | Val loss: 19.3711\n",
      "\n",
      "Epoch: 454 |  Train loss: 5.9306 | Val loss: 18.4639\n",
      "\n",
      "Epoch: 455 |  Train loss: 5.8946 | Val loss: 19.2615\n",
      "\n",
      "Epoch: 456 |  Train loss: 5.8985 | Val loss: 19.4775\n",
      "\n",
      "Epoch: 457 |  Train loss: 5.8996 | Val loss: 19.6861\n",
      "\n",
      "Epoch: 458 |  Train loss: 6.0344 | Val loss: 18.5749\n",
      "\n",
      "Epoch: 459 |  Train loss: 6.0468 | Val loss: 20.8477\n",
      "\n",
      "Epoch: 460 |  Train loss: 6.2277 | Val loss: 19.5571\n",
      "\n",
      "Epoch: 461 |  Train loss: 6.0558 | Val loss: 21.7198\n",
      "\n",
      "Epoch: 462 |  Train loss: 5.9118 | Val loss: 21.0732\n",
      "\n",
      "Epoch: 463 |  Train loss: 5.8598 | Val loss: 19.9909\n",
      "\n",
      "Epoch: 464 |  Train loss: 5.8404 | Val loss: 19.2795\n",
      "\n",
      "Epoch: 465 |  Train loss: 5.8466 | Val loss: 20.2338\n",
      "\n",
      "Epoch: 466 |  Train loss: 5.7735 | Val loss: 19.1752\n",
      "\n",
      "Epoch: 467 |  Train loss: 5.7480 | Val loss: 19.7897\n",
      "\n",
      "Epoch: 468 |  Train loss: 5.7709 | Val loss: 18.5225\n",
      "\n",
      "Epoch: 469 |  Train loss: 6.0409 | Val loss: 20.3496\n",
      "\n",
      "Epoch: 470 |  Train loss: 5.9876 | Val loss: 18.5575\n",
      "\n",
      "Epoch: 471 |  Train loss: 6.0466 | Val loss: 21.2960\n",
      "\n",
      "Epoch: 472 |  Train loss: 5.9516 | Val loss: 18.1434\n",
      "\n",
      "Epoch: 473 |  Train loss: 5.8855 | Val loss: 20.5209\n",
      "\n",
      "Epoch: 474 |  Train loss: 5.8959 | Val loss: 19.0385\n",
      "\n",
      "Epoch: 475 |  Train loss: 5.8073 | Val loss: 18.9553\n",
      "\n",
      "Epoch: 476 |  Train loss: 5.8666 | Val loss: 19.9419\n",
      "\n",
      "Epoch: 477 |  Train loss: 6.2834 | Val loss: 19.1757\n",
      "\n",
      "Epoch: 478 |  Train loss: 6.0750 | Val loss: 19.4470\n",
      "\n",
      "Epoch: 479 |  Train loss: 5.9592 | Val loss: 19.7999\n",
      "\n",
      "Epoch: 480 |  Train loss: 6.0155 | Val loss: 17.5002\n",
      "\n",
      "Epoch: 481 |  Train loss: 6.0252 | Val loss: 19.8621\n",
      "\n",
      "Epoch: 482 |  Train loss: 5.8364 | Val loss: 19.9936\n",
      "\n",
      "Epoch: 483 |  Train loss: 5.7627 | Val loss: 18.6155\n",
      "\n",
      "Epoch: 484 |  Train loss: 5.7546 | Val loss: 18.6801\n",
      "\n",
      "Epoch: 485 |  Train loss: 5.9103 | Val loss: 18.2715\n",
      "\n",
      "Epoch: 486 |  Train loss: 5.9336 | Val loss: 19.6341\n",
      "\n",
      "Epoch: 487 |  Train loss: 6.5921 | Val loss: 19.1600\n",
      "\n",
      "Epoch: 488 |  Train loss: 6.1376 | Val loss: 18.4184\n",
      "\n",
      "Epoch: 489 |  Train loss: 5.9877 | Val loss: 17.9016\n",
      "\n",
      "Epoch: 490 |  Train loss: 5.8300 | Val loss: 18.4448\n",
      "\n",
      "Epoch: 491 |  Train loss: 5.8204 | Val loss: 18.9022\n",
      "\n",
      "Epoch: 492 |  Train loss: 5.9119 | Val loss: 18.5243\n",
      "\n",
      "Epoch: 493 |  Train loss: 5.9046 | Val loss: 19.9072\n",
      "\n",
      "Epoch: 494 |  Train loss: 6.0463 | Val loss: 20.4407\n",
      "\n",
      "Epoch: 495 |  Train loss: 5.8082 | Val loss: 18.3312\n",
      "\n",
      "Epoch: 496 |  Train loss: 5.8469 | Val loss: 19.6399\n",
      "\n",
      "Epoch: 497 |  Train loss: 5.8867 | Val loss: 17.6790\n",
      "\n",
      "Epoch: 498 |  Train loss: 6.0441 | Val loss: 18.4919\n",
      "\n",
      "Epoch: 499 |  Train loss: 5.8568 | Val loss: 17.9752\n",
      "\n",
      "Iteration: 6000/12000 | loss = 2.3320\n",
      "Epoch: 500 |  Train loss: 5.8207 | Val loss: 20.0313\n",
      "\n",
      "Epoch: 501 |  Train loss: 5.7846 | Val loss: 17.9271\n",
      "\n",
      "Epoch: 502 |  Train loss: 5.7204 | Val loss: 19.1883\n",
      "\n",
      "Epoch: 503 |  Train loss: 5.7570 | Val loss: 18.4486\n",
      "\n",
      "Epoch: 504 |  Train loss: 5.8186 | Val loss: 19.9612\n",
      "\n",
      "Epoch: 505 |  Train loss: 5.7188 | Val loss: 19.4267\n",
      "\n",
      "Epoch: 506 |  Train loss: 5.7796 | Val loss: 18.6839\n",
      "\n",
      "Epoch: 507 |  Train loss: 5.8802 | Val loss: 19.0470\n",
      "\n",
      "Epoch: 508 |  Train loss: 5.7450 | Val loss: 18.3797\n",
      "\n",
      "Epoch: 509 |  Train loss: 5.9607 | Val loss: 18.0531\n",
      "\n",
      "Epoch: 510 |  Train loss: 5.9872 | Val loss: 20.5311\n",
      "\n",
      "Epoch: 511 |  Train loss: 6.1119 | Val loss: 18.8711\n",
      "\n",
      "Epoch: 512 |  Train loss: 6.4886 | Val loss: 20.0836\n",
      "\n",
      "Epoch: 513 |  Train loss: 6.0464 | Val loss: 18.8938\n",
      "\n",
      "Epoch: 514 |  Train loss: 5.9576 | Val loss: 19.5730\n",
      "\n",
      "Epoch: 515 |  Train loss: 5.8295 | Val loss: 19.2774\n",
      "\n",
      "Epoch: 516 |  Train loss: 5.6984 | Val loss: 19.2460\n",
      "\n",
      "Epoch: 517 |  Train loss: 6.0185 | Val loss: 18.9808\n",
      "\n",
      "Epoch: 518 |  Train loss: 6.2699 | Val loss: 17.7656\n",
      "\n",
      "Epoch: 519 |  Train loss: 5.9155 | Val loss: 18.1173\n",
      "\n",
      "Epoch: 520 |  Train loss: 5.8180 | Val loss: 19.3725\n",
      "\n",
      "Epoch: 521 |  Train loss: 5.7841 | Val loss: 19.8225\n",
      "\n",
      "Epoch: 522 |  Train loss: 5.7151 | Val loss: 18.9018\n",
      "\n",
      "Epoch: 523 |  Train loss: 5.9365 | Val loss: 18.5927\n",
      "\n",
      "Epoch: 524 |  Train loss: 5.7427 | Val loss: 18.9008\n",
      "\n",
      "Epoch: 525 |  Train loss: 5.6822 | Val loss: 18.3896\n",
      "\n",
      "Epoch: 526 |  Train loss: 5.7237 | Val loss: 19.9344\n",
      "\n",
      "Epoch: 527 |  Train loss: 5.8911 | Val loss: 18.2692\n",
      "\n",
      "Epoch: 528 |  Train loss: 5.9111 | Val loss: 20.7021\n",
      "\n",
      "Epoch: 529 |  Train loss: 6.0237 | Val loss: 20.3726\n",
      "\n",
      "Epoch: 530 |  Train loss: 5.8070 | Val loss: 18.9869\n",
      "\n",
      "Epoch: 531 |  Train loss: 5.8337 | Val loss: 19.0182\n",
      "\n",
      "Epoch: 532 |  Train loss: 5.8272 | Val loss: 19.7473\n",
      "\n",
      "Epoch: 533 |  Train loss: 5.6663 | Val loss: 19.4078\n",
      "\n",
      "Epoch: 534 |  Train loss: 5.7698 | Val loss: 19.2796\n",
      "\n",
      "Epoch: 535 |  Train loss: 5.6542 | Val loss: 19.3843\n",
      "\n",
      "Epoch: 536 |  Train loss: 5.7291 | Val loss: 18.2141\n",
      "\n",
      "Epoch: 537 |  Train loss: 6.1077 | Val loss: 19.4450\n",
      "\n",
      "Epoch: 538 |  Train loss: 5.8672 | Val loss: 17.2305\n",
      "\n",
      "Epoch: 539 |  Train loss: 5.7036 | Val loss: 18.4119\n",
      "\n",
      "Epoch: 540 |  Train loss: 5.8236 | Val loss: 18.8717\n",
      "\n",
      "Epoch: 541 |  Train loss: 5.7031 | Val loss: 18.2388\n",
      "\n",
      "Epoch: 542 |  Train loss: 5.6720 | Val loss: 18.5638\n",
      "\n",
      "Epoch: 543 |  Train loss: 5.6507 | Val loss: 18.5170\n",
      "\n",
      "Epoch: 544 |  Train loss: 5.6097 | Val loss: 18.1946\n",
      "\n",
      "Epoch: 545 |  Train loss: 5.6646 | Val loss: 18.6107\n",
      "\n",
      "Epoch: 546 |  Train loss: 5.8343 | Val loss: 18.3671\n",
      "\n",
      "Epoch: 547 |  Train loss: 5.6769 | Val loss: 18.6743\n",
      "\n",
      "Epoch: 548 |  Train loss: 5.6397 | Val loss: 18.7751\n",
      "\n",
      "Epoch: 549 |  Train loss: 5.6187 | Val loss: 17.8511\n",
      "\n",
      "Epoch: 550 |  Train loss: 5.7941 | Val loss: 19.3133\n",
      "\n",
      "Epoch: 551 |  Train loss: 5.9986 | Val loss: 17.3223\n",
      "\n",
      "Epoch: 552 |  Train loss: 5.7373 | Val loss: 19.0114\n",
      "\n",
      "Epoch: 553 |  Train loss: 5.6698 | Val loss: 18.7642\n",
      "\n",
      "Epoch: 554 |  Train loss: 5.6356 | Val loss: 19.0996\n",
      "\n",
      "Epoch: 555 |  Train loss: 6.4022 | Val loss: 19.1388\n",
      "\n",
      "Epoch: 556 |  Train loss: 6.1026 | Val loss: 18.6257\n",
      "\n",
      "Epoch: 557 |  Train loss: 5.9574 | Val loss: 18.5699\n",
      "\n",
      "Epoch: 558 |  Train loss: 5.9580 | Val loss: 20.0697\n",
      "\n",
      "Epoch: 559 |  Train loss: 5.9282 | Val loss: 16.9552\n",
      "\n",
      "Epoch: 560 |  Train loss: 5.6234 | Val loss: 18.6857\n",
      "\n",
      "Epoch: 561 |  Train loss: 5.6061 | Val loss: 18.3971\n",
      "\n",
      "Epoch: 562 |  Train loss: 5.6459 | Val loss: 18.5318\n",
      "\n",
      "Epoch: 563 |  Train loss: 5.9992 | Val loss: 17.6261\n",
      "\n",
      "Epoch: 564 |  Train loss: 6.4931 | Val loss: 22.1096\n",
      "\n",
      "Epoch: 565 |  Train loss: 6.3634 | Val loss: 16.9210\n",
      "\n",
      "Epoch: 566 |  Train loss: 5.9409 | Val loss: 19.8128\n",
      "\n",
      "Epoch: 567 |  Train loss: 5.6471 | Val loss: 17.8790\n",
      "\n",
      "Epoch: 568 |  Train loss: 5.6394 | Val loss: 18.5386\n",
      "\n",
      "Epoch: 569 |  Train loss: 6.2888 | Val loss: 20.3996\n",
      "\n",
      "Epoch: 570 |  Train loss: 6.0290 | Val loss: 17.8666\n",
      "\n",
      "Epoch: 571 |  Train loss: 5.7974 | Val loss: 18.5345\n",
      "\n",
      "Epoch: 572 |  Train loss: 5.6657 | Val loss: 18.1746\n",
      "\n",
      "Epoch: 573 |  Train loss: 5.6664 | Val loss: 19.3029\n",
      "\n",
      "Epoch: 574 |  Train loss: 5.7986 | Val loss: 18.5855\n",
      "\n",
      "Epoch: 575 |  Train loss: 5.9633 | Val loss: 18.1625\n",
      "\n",
      "Epoch: 576 |  Train loss: 5.6929 | Val loss: 19.9682\n",
      "\n",
      "Epoch: 577 |  Train loss: 6.0872 | Val loss: 19.1886\n",
      "\n",
      "Epoch: 578 |  Train loss: 5.8681 | Val loss: 18.6477\n",
      "\n",
      "Epoch: 579 |  Train loss: 5.7964 | Val loss: 18.2527\n",
      "\n",
      "Epoch: 580 |  Train loss: 5.9335 | Val loss: 20.0517\n",
      "\n",
      "Epoch: 581 |  Train loss: 5.6710 | Val loss: 18.5912\n",
      "\n",
      "Epoch: 582 |  Train loss: 5.5770 | Val loss: 18.0395\n",
      "\n",
      "Epoch: 583 |  Train loss: 5.5439 | Val loss: 18.8543\n",
      "\n",
      "Iteration: 7000/12000 | loss = 4.2068\n",
      "Epoch: 584 |  Train loss: 5.6082 | Val loss: 18.4061\n",
      "\n",
      "Epoch: 585 |  Train loss: 5.6951 | Val loss: 18.8597\n",
      "\n",
      "Epoch: 586 |  Train loss: 5.6815 | Val loss: 18.2132\n",
      "\n",
      "Epoch: 587 |  Train loss: 5.6402 | Val loss: 19.2165\n",
      "\n",
      "Epoch: 588 |  Train loss: 5.6777 | Val loss: 18.5674\n",
      "\n",
      "Epoch: 589 |  Train loss: 5.5801 | Val loss: 19.7397\n",
      "\n",
      "Epoch: 590 |  Train loss: 5.5254 | Val loss: 18.7999\n",
      "\n",
      "Epoch: 591 |  Train loss: 5.5304 | Val loss: 19.5853\n",
      "\n",
      "Epoch: 592 |  Train loss: 5.4837 | Val loss: 18.5549\n",
      "\n",
      "Epoch: 593 |  Train loss: 5.7218 | Val loss: 19.0510\n",
      "\n",
      "Epoch: 594 |  Train loss: 5.7402 | Val loss: 20.0855\n",
      "\n",
      "Epoch: 595 |  Train loss: 6.1178 | Val loss: 17.8806\n",
      "\n",
      "Epoch: 596 |  Train loss: 6.1238 | Val loss: 22.2000\n",
      "\n",
      "Epoch: 597 |  Train loss: 6.1270 | Val loss: 17.9233\n",
      "\n",
      "Epoch: 598 |  Train loss: 5.7926 | Val loss: 21.3853\n",
      "\n",
      "Epoch: 599 |  Train loss: 5.8333 | Val loss: 17.7901\n",
      "\n",
      "Epoch: 600 |  Train loss: 5.9420 | Val loss: 21.6184\n",
      "\n",
      "Epoch: 601 |  Train loss: 5.8627 | Val loss: 18.4398\n",
      "\n",
      "Epoch: 602 |  Train loss: 5.6319 | Val loss: 20.4657\n",
      "\n",
      "Epoch: 603 |  Train loss: 5.6685 | Val loss: 18.4215\n",
      "\n",
      "Epoch: 604 |  Train loss: 5.5561 | Val loss: 20.0321\n",
      "\n",
      "Epoch: 605 |  Train loss: 5.6455 | Val loss: 19.0395\n",
      "\n",
      "Epoch: 606 |  Train loss: 5.6832 | Val loss: 19.9634\n",
      "\n",
      "Epoch: 607 |  Train loss: 5.7371 | Val loss: 19.5485\n",
      "\n",
      "Epoch: 608 |  Train loss: 5.6263 | Val loss: 19.0318\n",
      "\n",
      "Epoch: 609 |  Train loss: 5.5808 | Val loss: 18.1492\n",
      "\n",
      "Epoch: 610 |  Train loss: 5.4888 | Val loss: 17.9167\n",
      "\n",
      "Epoch: 611 |  Train loss: 5.6721 | Val loss: 20.0062\n",
      "\n",
      "Epoch: 612 |  Train loss: 5.5327 | Val loss: 19.0980\n",
      "\n",
      "Epoch: 613 |  Train loss: 5.6794 | Val loss: 18.8354\n",
      "\n",
      "Epoch: 614 |  Train loss: 5.9004 | Val loss: 18.1879\n",
      "\n",
      "Epoch: 615 |  Train loss: 5.6055 | Val loss: 17.7055\n",
      "\n",
      "Epoch: 616 |  Train loss: 5.5612 | Val loss: 19.0523\n",
      "\n",
      "Epoch: 617 |  Train loss: 5.8743 | Val loss: 17.7944\n",
      "\n",
      "Epoch: 618 |  Train loss: 5.5800 | Val loss: 19.6346\n",
      "\n",
      "Epoch: 619 |  Train loss: 5.5162 | Val loss: 18.2835\n",
      "\n",
      "Epoch: 620 |  Train loss: 5.4888 | Val loss: 18.8737\n",
      "\n",
      "Epoch: 621 |  Train loss: 5.4897 | Val loss: 18.5511\n",
      "\n",
      "Epoch: 622 |  Train loss: 5.6837 | Val loss: 20.1207\n",
      "\n",
      "Epoch: 623 |  Train loss: 5.6611 | Val loss: 18.6134\n",
      "\n",
      "Epoch: 624 |  Train loss: 5.4978 | Val loss: 19.3203\n",
      "\n",
      "Epoch: 625 |  Train loss: 5.5350 | Val loss: 19.3831\n",
      "\n",
      "Epoch: 626 |  Train loss: 5.5350 | Val loss: 18.0670\n",
      "\n",
      "Epoch: 627 |  Train loss: 5.5379 | Val loss: 18.0930\n",
      "\n",
      "Epoch: 628 |  Train loss: 5.9037 | Val loss: 20.4577\n",
      "\n",
      "Epoch: 629 |  Train loss: 6.0604 | Val loss: 17.9118\n",
      "\n",
      "Epoch: 630 |  Train loss: 6.0863 | Val loss: 21.3032\n",
      "\n",
      "Epoch: 631 |  Train loss: 5.7158 | Val loss: 17.4093\n",
      "\n",
      "Epoch: 632 |  Train loss: 5.5161 | Val loss: 18.8498\n",
      "\n",
      "Epoch: 633 |  Train loss: 5.6291 | Val loss: 17.7022\n",
      "\n",
      "Epoch: 634 |  Train loss: 5.5449 | Val loss: 19.1229\n",
      "\n",
      "Epoch: 635 |  Train loss: 5.4549 | Val loss: 18.0974\n",
      "\n",
      "Epoch: 636 |  Train loss: 5.4887 | Val loss: 18.2512\n",
      "\n",
      "Epoch: 637 |  Train loss: 6.1195 | Val loss: 19.0975\n",
      "\n",
      "Epoch: 638 |  Train loss: 5.6788 | Val loss: 18.7733\n",
      "\n",
      "Epoch: 639 |  Train loss: 5.4720 | Val loss: 19.3413\n",
      "\n",
      "Epoch: 640 |  Train loss: 5.5780 | Val loss: 18.1353\n",
      "\n",
      "Epoch: 641 |  Train loss: 5.8636 | Val loss: 20.6254\n",
      "\n",
      "Epoch: 642 |  Train loss: 5.5125 | Val loss: 17.3535\n",
      "\n",
      "Epoch: 643 |  Train loss: 5.6112 | Val loss: 18.2034\n",
      "\n",
      "Epoch: 644 |  Train loss: 5.4899 | Val loss: 19.4855\n",
      "\n",
      "Epoch: 645 |  Train loss: 5.5573 | Val loss: 17.7077\n",
      "\n",
      "Epoch: 646 |  Train loss: 5.6650 | Val loss: 20.5120\n",
      "\n",
      "Epoch: 647 |  Train loss: 6.3599 | Val loss: 19.8216\n",
      "\n",
      "Epoch: 648 |  Train loss: 5.7336 | Val loss: 17.9008\n",
      "\n",
      "Epoch: 649 |  Train loss: 5.9922 | Val loss: 21.3039\n",
      "\n",
      "Epoch: 650 |  Train loss: 5.6700 | Val loss: 17.7827\n",
      "\n",
      "Epoch: 651 |  Train loss: 5.5720 | Val loss: 19.4158\n",
      "\n",
      "Epoch: 652 |  Train loss: 5.7151 | Val loss: 19.0052\n",
      "\n",
      "Epoch: 653 |  Train loss: 5.5835 | Val loss: 17.7929\n",
      "\n",
      "Epoch: 654 |  Train loss: 5.6583 | Val loss: 18.4650\n",
      "\n",
      "Epoch: 655 |  Train loss: 5.5108 | Val loss: 18.8295\n",
      "\n",
      "Epoch: 656 |  Train loss: 5.5043 | Val loss: 18.3847\n",
      "\n",
      "Epoch: 657 |  Train loss: 5.4366 | Val loss: 18.0473\n",
      "\n",
      "Epoch: 658 |  Train loss: 5.4943 | Val loss: 18.8345\n",
      "\n",
      "Epoch: 659 |  Train loss: 5.5249 | Val loss: 17.9450\n",
      "\n",
      "Epoch: 660 |  Train loss: 5.4280 | Val loss: 18.7025\n",
      "\n",
      "Epoch: 661 |  Train loss: 5.8504 | Val loss: 18.8647\n",
      "\n",
      "Epoch: 662 |  Train loss: 6.2039 | Val loss: 23.2159\n",
      "\n",
      "Epoch: 663 |  Train loss: 5.5341 | Val loss: 19.1535\n",
      "\n",
      "Epoch: 664 |  Train loss: 5.4554 | Val loss: 18.3177\n",
      "\n",
      "Epoch: 665 |  Train loss: 5.4502 | Val loss: 17.2531\n",
      "\n",
      "Epoch: 666 |  Train loss: 5.4190 | Val loss: 18.1571\n",
      "\n",
      "Iteration: 8000/12000 | loss = 4.0901\n",
      "Epoch: 667 |  Train loss: 5.4127 | Val loss: 18.2484\n",
      "\n",
      "Epoch: 668 |  Train loss: 5.4335 | Val loss: 18.5525\n",
      "\n",
      "Epoch: 669 |  Train loss: 5.3894 | Val loss: 18.9041\n",
      "\n",
      "Epoch: 670 |  Train loss: 5.5991 | Val loss: 18.4703\n",
      "\n",
      "Epoch: 671 |  Train loss: 5.5272 | Val loss: 19.3150\n",
      "\n",
      "Epoch: 672 |  Train loss: 5.9355 | Val loss: 17.3467\n",
      "\n",
      "Epoch: 673 |  Train loss: 5.5710 | Val loss: 18.1720\n",
      "\n",
      "Epoch: 674 |  Train loss: 5.5841 | Val loss: 17.6780\n",
      "\n",
      "Epoch: 675 |  Train loss: 5.4340 | Val loss: 17.1775\n",
      "\n",
      "Epoch: 676 |  Train loss: 5.4404 | Val loss: 19.0750\n",
      "\n",
      "Epoch: 677 |  Train loss: 5.4881 | Val loss: 18.9486\n",
      "\n",
      "Epoch: 678 |  Train loss: 5.4551 | Val loss: 17.6937\n",
      "\n",
      "Epoch: 679 |  Train loss: 6.1316 | Val loss: 18.4523\n",
      "\n",
      "Epoch: 680 |  Train loss: 5.9650 | Val loss: 17.4768\n",
      "\n",
      "Epoch: 681 |  Train loss: 5.8328 | Val loss: 18.6061\n",
      "\n",
      "Epoch: 682 |  Train loss: 5.5284 | Val loss: 17.2926\n",
      "\n",
      "Epoch: 683 |  Train loss: 6.0276 | Val loss: 17.5410\n",
      "\n",
      "Epoch: 684 |  Train loss: 5.5296 | Val loss: 17.5107\n",
      "\n",
      "Epoch: 685 |  Train loss: 5.6169 | Val loss: 19.2515\n",
      "\n",
      "Epoch: 686 |  Train loss: 5.7873 | Val loss: 17.5796\n",
      "\n",
      "Epoch: 687 |  Train loss: 5.5754 | Val loss: 19.3802\n",
      "\n",
      "Epoch: 688 |  Train loss: 5.5093 | Val loss: 17.9177\n",
      "\n",
      "Epoch: 689 |  Train loss: 5.4999 | Val loss: 18.7611\n",
      "\n",
      "Epoch: 690 |  Train loss: 5.5589 | Val loss: 16.8908\n",
      "\n",
      "Epoch: 691 |  Train loss: 5.5224 | Val loss: 17.8963\n",
      "\n",
      "Epoch: 692 |  Train loss: 5.5556 | Val loss: 17.9636\n",
      "\n",
      "Epoch: 693 |  Train loss: 5.3928 | Val loss: 17.8820\n",
      "\n",
      "Epoch: 694 |  Train loss: 5.4315 | Val loss: 18.0433\n",
      "\n",
      "Epoch: 695 |  Train loss: 5.4614 | Val loss: 18.4999\n",
      "\n",
      "Epoch: 696 |  Train loss: 5.3793 | Val loss: 17.5144\n",
      "\n",
      "Epoch: 697 |  Train loss: 5.6050 | Val loss: 17.7473\n",
      "\n",
      "Epoch: 698 |  Train loss: 5.4279 | Val loss: 18.5575\n",
      "\n",
      "Epoch: 699 |  Train loss: 5.3755 | Val loss: 18.4290\n",
      "\n",
      "Epoch: 700 |  Train loss: 5.5663 | Val loss: 18.6564\n",
      "\n",
      "Epoch: 701 |  Train loss: 5.5675 | Val loss: 17.3803\n",
      "\n",
      "Epoch: 702 |  Train loss: 5.4736 | Val loss: 18.4546\n",
      "\n",
      "Epoch: 703 |  Train loss: 5.3782 | Val loss: 17.9131\n",
      "\n",
      "Epoch: 704 |  Train loss: 5.4122 | Val loss: 18.9034\n",
      "\n",
      "Epoch: 705 |  Train loss: 5.3752 | Val loss: 18.2114\n",
      "\n",
      "Epoch: 706 |  Train loss: 5.4266 | Val loss: 19.3094\n",
      "\n",
      "Epoch: 707 |  Train loss: 5.3058 | Val loss: 18.5703\n",
      "\n",
      "Epoch: 708 |  Train loss: 5.5262 | Val loss: 18.3998\n",
      "\n",
      "Epoch: 709 |  Train loss: 5.5734 | Val loss: 19.3961\n",
      "\n",
      "Epoch: 710 |  Train loss: 5.3794 | Val loss: 18.6785\n",
      "\n",
      "Epoch: 711 |  Train loss: 5.4701 | Val loss: 19.4721\n",
      "\n",
      "Epoch: 712 |  Train loss: 5.5755 | Val loss: 17.6619\n",
      "\n",
      "Epoch: 713 |  Train loss: 5.4712 | Val loss: 17.9108\n",
      "\n",
      "Epoch: 714 |  Train loss: 5.3596 | Val loss: 18.1397\n",
      "\n",
      "Epoch: 715 |  Train loss: 5.4395 | Val loss: 18.4856\n",
      "\n",
      "Epoch: 716 |  Train loss: 5.6596 | Val loss: 21.0808\n",
      "\n",
      "Epoch: 717 |  Train loss: 5.8160 | Val loss: 16.9254\n",
      "\n",
      "Epoch: 718 |  Train loss: 5.6294 | Val loss: 17.6711\n",
      "\n",
      "Epoch: 719 |  Train loss: 5.6806 | Val loss: 17.9457\n",
      "\n",
      "Epoch: 720 |  Train loss: 5.4659 | Val loss: 16.5573\n",
      "\n",
      "Epoch: 721 |  Train loss: 5.5298 | Val loss: 18.2414\n",
      "\n",
      "Epoch: 722 |  Train loss: 5.5571 | Val loss: 18.9023\n",
      "\n",
      "Epoch: 723 |  Train loss: 5.4880 | Val loss: 18.1107\n",
      "\n",
      "Epoch: 724 |  Train loss: 5.6321 | Val loss: 19.7160\n",
      "\n",
      "Epoch: 725 |  Train loss: 5.3664 | Val loss: 17.7564\n",
      "\n",
      "Epoch: 726 |  Train loss: 5.3764 | Val loss: 17.9931\n",
      "\n",
      "Epoch: 727 |  Train loss: 5.4275 | Val loss: 19.4014\n",
      "\n",
      "Epoch: 728 |  Train loss: 5.6313 | Val loss: 17.9219\n",
      "\n",
      "Epoch: 729 |  Train loss: 5.8399 | Val loss: 20.3398\n",
      "\n",
      "Epoch: 730 |  Train loss: 5.5008 | Val loss: 17.1094\n",
      "\n",
      "Epoch: 731 |  Train loss: 5.7393 | Val loss: 18.9675\n",
      "\n",
      "Epoch: 732 |  Train loss: 5.3658 | Val loss: 17.4732\n",
      "\n",
      "Epoch: 733 |  Train loss: 5.3291 | Val loss: 18.4970\n",
      "\n",
      "Epoch: 734 |  Train loss: 5.3474 | Val loss: 17.9215\n",
      "\n",
      "Epoch: 735 |  Train loss: 5.7132 | Val loss: 19.9543\n",
      "\n",
      "Epoch: 736 |  Train loss: 6.0254 | Val loss: 17.0991\n",
      "\n",
      "Epoch: 737 |  Train loss: 6.0086 | Val loss: 21.1097\n",
      "\n",
      "Epoch: 738 |  Train loss: 5.6314 | Val loss: 17.9057\n",
      "\n",
      "Epoch: 739 |  Train loss: 5.4133 | Val loss: 19.4128\n",
      "\n",
      "Epoch: 740 |  Train loss: 5.2792 | Val loss: 18.6280\n",
      "\n",
      "Epoch: 741 |  Train loss: 5.4477 | Val loss: 17.4146\n",
      "\n",
      "Epoch: 742 |  Train loss: 5.4023 | Val loss: 19.9502\n",
      "\n",
      "Epoch: 743 |  Train loss: 5.5125 | Val loss: 17.9274\n",
      "\n",
      "Epoch: 744 |  Train loss: 5.3718 | Val loss: 19.3004\n",
      "\n",
      "Epoch: 745 |  Train loss: 5.4208 | Val loss: 19.1422\n",
      "\n",
      "Epoch: 746 |  Train loss: 5.4375 | Val loss: 17.7276\n",
      "\n",
      "Epoch: 747 |  Train loss: 5.5507 | Val loss: 19.2953\n",
      "\n",
      "Epoch: 748 |  Train loss: 5.3620 | Val loss: 17.9712\n",
      "\n",
      "Epoch: 749 |  Train loss: 5.3825 | Val loss: 17.6320\n",
      "\n",
      "Iteration: 9000/12000 | loss = 3.2743\n",
      "Epoch: 750 |  Train loss: 5.4620 | Val loss: 20.6701\n",
      "\n",
      "Epoch: 751 |  Train loss: 5.3509 | Val loss: 17.2710\n",
      "\n",
      "Epoch: 752 |  Train loss: 5.4897 | Val loss: 20.1009\n",
      "\n",
      "Epoch: 753 |  Train loss: 5.3125 | Val loss: 17.8989\n",
      "\n",
      "Epoch: 754 |  Train loss: 5.3570 | Val loss: 19.6446\n",
      "\n",
      "Epoch: 755 |  Train loss: 5.6075 | Val loss: 16.7621\n",
      "\n",
      "Epoch: 756 |  Train loss: 5.5035 | Val loss: 20.4068\n",
      "\n",
      "Epoch: 757 |  Train loss: 5.4978 | Val loss: 20.3818\n",
      "\n",
      "Epoch: 758 |  Train loss: 5.3850 | Val loss: 18.0374\n",
      "\n",
      "Epoch: 759 |  Train loss: 5.4132 | Val loss: 19.1255\n",
      "\n",
      "Epoch: 760 |  Train loss: 5.4720 | Val loss: 19.5644\n",
      "\n",
      "Epoch: 761 |  Train loss: 5.3622 | Val loss: 18.6618\n",
      "\n",
      "Epoch: 762 |  Train loss: 5.2684 | Val loss: 18.3991\n",
      "\n",
      "Epoch: 763 |  Train loss: 5.5074 | Val loss: 17.6208\n",
      "\n",
      "Epoch: 764 |  Train loss: 5.3510 | Val loss: 18.2592\n",
      "\n",
      "Epoch: 765 |  Train loss: 5.3598 | Val loss: 18.5458\n",
      "\n",
      "Epoch: 766 |  Train loss: 5.2829 | Val loss: 18.6028\n",
      "\n",
      "Epoch: 767 |  Train loss: 5.2553 | Val loss: 18.2273\n",
      "\n",
      "Epoch: 768 |  Train loss: 5.3076 | Val loss: 18.6632\n",
      "\n",
      "Epoch: 769 |  Train loss: 5.3678 | Val loss: 17.3977\n",
      "\n",
      "Epoch: 770 |  Train loss: 5.3202 | Val loss: 18.9967\n",
      "\n",
      "Epoch: 771 |  Train loss: 5.2636 | Val loss: 17.8047\n",
      "\n",
      "Epoch: 772 |  Train loss: 5.2406 | Val loss: 18.6965\n",
      "\n",
      "Epoch: 773 |  Train loss: 5.5643 | Val loss: 20.3294\n",
      "\n",
      "Epoch: 774 |  Train loss: 5.7231 | Val loss: 18.0522\n",
      "\n",
      "Epoch: 775 |  Train loss: 5.2849 | Val loss: 18.4157\n",
      "\n",
      "Epoch: 776 |  Train loss: 5.4706 | Val loss: 19.3602\n",
      "\n",
      "Epoch: 777 |  Train loss: 5.3495 | Val loss: 17.8846\n",
      "\n",
      "Epoch: 778 |  Train loss: 5.6622 | Val loss: 19.6757\n",
      "\n",
      "Epoch: 779 |  Train loss: 5.5006 | Val loss: 18.2243\n",
      "\n",
      "Epoch: 780 |  Train loss: 5.3401 | Val loss: 17.7142\n",
      "\n",
      "Epoch: 781 |  Train loss: 5.3114 | Val loss: 17.1124\n",
      "\n",
      "Epoch: 782 |  Train loss: 5.4946 | Val loss: 18.5176\n",
      "\n",
      "Epoch: 783 |  Train loss: 5.2698 | Val loss: 18.3020\n",
      "\n",
      "Epoch: 784 |  Train loss: 5.2964 | Val loss: 18.0735\n",
      "\n",
      "Epoch: 785 |  Train loss: 5.2341 | Val loss: 18.6780\n",
      "\n",
      "Epoch: 786 |  Train loss: 5.3772 | Val loss: 17.7910\n",
      "\n",
      "Epoch: 787 |  Train loss: 5.4218 | Val loss: 18.9495\n",
      "\n",
      "Epoch: 788 |  Train loss: 5.3761 | Val loss: 18.0895\n",
      "\n",
      "Epoch: 789 |  Train loss: 5.3332 | Val loss: 18.9512\n",
      "\n",
      "Epoch: 790 |  Train loss: 5.2194 | Val loss: 17.9544\n",
      "\n",
      "Epoch: 791 |  Train loss: 5.6613 | Val loss: 18.5644\n",
      "\n",
      "Epoch: 792 |  Train loss: 5.3400 | Val loss: 18.4278\n",
      "\n",
      "Epoch: 793 |  Train loss: 5.2418 | Val loss: 18.7268\n",
      "\n",
      "Epoch: 794 |  Train loss: 5.7798 | Val loss: 18.2231\n",
      "\n",
      "Epoch: 795 |  Train loss: 5.9216 | Val loss: 18.3168\n",
      "\n",
      "Epoch: 796 |  Train loss: 6.1136 | Val loss: 19.4153\n",
      "\n",
      "Epoch: 797 |  Train loss: 5.4548 | Val loss: 19.0823\n",
      "\n",
      "Epoch: 798 |  Train loss: 5.3798 | Val loss: 18.0904\n",
      "\n",
      "Epoch: 799 |  Train loss: 5.2040 | Val loss: 17.7516\n",
      "\n",
      "Epoch: 800 |  Train loss: 5.3123 | Val loss: 18.8508\n",
      "\n",
      "Epoch: 801 |  Train loss: 5.7317 | Val loss: 18.3471\n",
      "\n",
      "Epoch: 802 |  Train loss: 5.4592 | Val loss: 19.1699\n",
      "\n",
      "Epoch: 803 |  Train loss: 5.4371 | Val loss: 17.6144\n",
      "\n",
      "Epoch: 804 |  Train loss: 5.4071 | Val loss: 19.6652\n",
      "\n",
      "Epoch: 805 |  Train loss: 5.3143 | Val loss: 17.5120\n",
      "\n",
      "Epoch: 806 |  Train loss: 5.2348 | Val loss: 18.2161\n",
      "\n",
      "Epoch: 807 |  Train loss: 5.2692 | Val loss: 18.1776\n",
      "\n",
      "Epoch: 808 |  Train loss: 5.3235 | Val loss: 17.1821\n",
      "\n",
      "Epoch: 809 |  Train loss: 5.2456 | Val loss: 18.4875\n",
      "\n",
      "Epoch: 810 |  Train loss: 5.8061 | Val loss: 17.4380\n",
      "\n",
      "Epoch: 811 |  Train loss: 6.3131 | Val loss: 22.6544\n",
      "\n",
      "Epoch: 812 |  Train loss: 6.4534 | Val loss: 18.7014\n",
      "\n",
      "Epoch: 813 |  Train loss: 5.7357 | Val loss: 19.0589\n",
      "\n",
      "Epoch: 814 |  Train loss: 5.7106 | Val loss: 16.9462\n",
      "\n",
      "Epoch: 815 |  Train loss: 5.5255 | Val loss: 18.0630\n",
      "\n",
      "Epoch: 816 |  Train loss: 5.6615 | Val loss: 16.2655\n",
      "\n",
      "Epoch: 817 |  Train loss: 5.3896 | Val loss: 18.8863\n",
      "\n",
      "Epoch: 818 |  Train loss: 5.3820 | Val loss: 18.5086\n",
      "\n",
      "Epoch: 819 |  Train loss: 5.3551 | Val loss: 18.7007\n",
      "\n",
      "Epoch: 820 |  Train loss: 5.2435 | Val loss: 17.5141\n",
      "\n",
      "Epoch: 821 |  Train loss: 5.2742 | Val loss: 17.3910\n",
      "\n",
      "Epoch: 822 |  Train loss: 5.8565 | Val loss: 17.5576\n",
      "\n",
      "Epoch: 823 |  Train loss: 5.6402 | Val loss: 19.7025\n",
      "\n",
      "Epoch: 824 |  Train loss: 5.4182 | Val loss: 18.8904\n",
      "\n",
      "Epoch: 825 |  Train loss: 5.4000 | Val loss: 18.1512\n",
      "\n",
      "Epoch: 826 |  Train loss: 5.6900 | Val loss: 19.1543\n",
      "\n",
      "Epoch: 827 |  Train loss: 5.8911 | Val loss: 18.0854\n",
      "\n",
      "Epoch: 828 |  Train loss: 5.6600 | Val loss: 20.2667\n",
      "\n",
      "Epoch: 829 |  Train loss: 5.2650 | Val loss: 18.2465\n",
      "\n",
      "Epoch: 830 |  Train loss: 5.2355 | Val loss: 18.4526\n",
      "\n",
      "Epoch: 831 |  Train loss: 5.1658 | Val loss: 18.4324\n",
      "\n",
      "Epoch: 832 |  Train loss: 5.2010 | Val loss: 18.3013\n",
      "\n",
      "Epoch: 833 |  Train loss: 5.4490 | Val loss: 18.4085\n",
      "\n",
      "Iteration: 10000/12000 | loss = 3.4299\n",
      "Epoch: 834 |  Train loss: 5.3159 | Val loss: 18.0511\n",
      "\n",
      "Epoch: 835 |  Train loss: 5.6168 | Val loss: 19.7863\n",
      "\n",
      "Epoch: 836 |  Train loss: 5.3785 | Val loss: 18.7046\n",
      "\n",
      "Epoch: 837 |  Train loss: 5.5946 | Val loss: 19.5749\n",
      "\n",
      "Epoch: 838 |  Train loss: 5.3695 | Val loss: 17.9301\n",
      "\n",
      "Epoch: 839 |  Train loss: 5.3408 | Val loss: 19.5153\n",
      "\n",
      "Epoch: 840 |  Train loss: 5.2080 | Val loss: 19.3885\n",
      "\n",
      "Epoch: 841 |  Train loss: 5.2138 | Val loss: 19.5679\n",
      "\n",
      "Epoch: 842 |  Train loss: 5.2866 | Val loss: 18.5127\n",
      "\n",
      "Epoch: 843 |  Train loss: 5.1858 | Val loss: 18.3823\n",
      "\n",
      "Epoch: 844 |  Train loss: 5.1718 | Val loss: 19.0469\n",
      "\n",
      "Epoch: 845 |  Train loss: 5.1681 | Val loss: 18.3205\n",
      "\n",
      "Epoch: 846 |  Train loss: 5.2120 | Val loss: 18.9273\n",
      "\n",
      "Epoch: 847 |  Train loss: 5.2517 | Val loss: 18.3194\n",
      "\n",
      "Epoch: 848 |  Train loss: 5.6593 | Val loss: 20.6528\n",
      "\n",
      "Epoch: 849 |  Train loss: 5.8953 | Val loss: 18.3961\n",
      "\n",
      "Epoch: 850 |  Train loss: 5.5108 | Val loss: 20.2675\n",
      "\n",
      "Epoch: 851 |  Train loss: 5.2285 | Val loss: 17.4031\n",
      "\n",
      "Epoch: 852 |  Train loss: 5.3333 | Val loss: 18.9992\n",
      "\n",
      "Epoch: 853 |  Train loss: 5.5114 | Val loss: 17.6112\n",
      "\n",
      "Epoch: 854 |  Train loss: 5.5405 | Val loss: 19.9476\n",
      "\n",
      "Epoch: 855 |  Train loss: 5.7149 | Val loss: 17.4295\n",
      "\n",
      "Epoch: 856 |  Train loss: 6.1081 | Val loss: 22.1954\n",
      "\n",
      "Epoch: 857 |  Train loss: 5.5218 | Val loss: 17.9039\n",
      "\n",
      "Epoch: 858 |  Train loss: 5.1659 | Val loss: 18.9274\n",
      "\n",
      "Epoch: 859 |  Train loss: 5.1765 | Val loss: 18.6911\n",
      "\n",
      "Epoch: 860 |  Train loss: 5.1250 | Val loss: 17.6620\n",
      "\n",
      "Epoch: 861 |  Train loss: 5.4573 | Val loss: 17.5067\n",
      "\n",
      "Epoch: 862 |  Train loss: 5.5222 | Val loss: 20.0565\n",
      "\n",
      "Epoch: 863 |  Train loss: 5.6505 | Val loss: 18.1749\n",
      "\n",
      "Epoch: 864 |  Train loss: 5.1968 | Val loss: 19.3960\n",
      "\n",
      "Epoch: 865 |  Train loss: 5.1544 | Val loss: 18.3235\n",
      "\n",
      "Epoch: 866 |  Train loss: 5.1970 | Val loss: 19.2159\n",
      "\n",
      "Epoch: 867 |  Train loss: 5.2431 | Val loss: 17.3271\n",
      "\n",
      "Epoch: 868 |  Train loss: 5.1789 | Val loss: 18.5348\n",
      "\n",
      "Epoch: 869 |  Train loss: 5.2646 | Val loss: 19.2012\n",
      "\n",
      "Epoch: 870 |  Train loss: 5.1063 | Val loss: 18.5319\n",
      "\n",
      "Epoch: 871 |  Train loss: 5.2123 | Val loss: 18.3357\n",
      "\n",
      "Epoch: 872 |  Train loss: 5.1918 | Val loss: 19.4599\n",
      "\n",
      "Epoch: 873 |  Train loss: 5.3159 | Val loss: 18.3930\n",
      "\n",
      "Epoch: 874 |  Train loss: 5.3360 | Val loss: 18.4418\n",
      "\n",
      "Epoch: 875 |  Train loss: 5.6307 | Val loss: 19.8905\n",
      "\n",
      "Epoch: 876 |  Train loss: 5.2233 | Val loss: 18.1367\n",
      "\n",
      "Epoch: 877 |  Train loss: 5.1951 | Val loss: 18.5996\n",
      "\n",
      "Epoch: 878 |  Train loss: 5.2082 | Val loss: 18.9960\n",
      "\n",
      "Epoch: 879 |  Train loss: 5.8046 | Val loss: 19.2303\n",
      "\n",
      "Epoch: 880 |  Train loss: 5.4979 | Val loss: 19.8388\n",
      "\n",
      "Epoch: 881 |  Train loss: 5.4902 | Val loss: 17.5632\n",
      "\n",
      "Epoch: 882 |  Train loss: 5.2280 | Val loss: 19.2602\n",
      "\n",
      "Epoch: 883 |  Train loss: 5.1176 | Val loss: 19.0361\n",
      "\n",
      "Epoch: 884 |  Train loss: 5.3663 | Val loss: 19.3335\n",
      "\n",
      "Epoch: 885 |  Train loss: 5.2693 | Val loss: 19.5566\n",
      "\n",
      "Epoch: 886 |  Train loss: 5.1653 | Val loss: 19.6497\n",
      "\n",
      "Epoch: 887 |  Train loss: 5.1709 | Val loss: 19.5687\n",
      "\n",
      "Epoch: 888 |  Train loss: 5.2861 | Val loss: 18.3877\n",
      "\n",
      "Epoch: 889 |  Train loss: 5.0645 | Val loss: 18.6891\n",
      "\n",
      "Epoch: 890 |  Train loss: 5.1285 | Val loss: 19.8377\n",
      "\n",
      "Epoch: 891 |  Train loss: 5.5929 | Val loss: 18.6070\n",
      "\n",
      "Epoch: 892 |  Train loss: 5.0588 | Val loss: 18.7944\n",
      "\n",
      "Epoch: 893 |  Train loss: 5.4165 | Val loss: 20.7410\n",
      "\n",
      "Epoch: 894 |  Train loss: 6.1835 | Val loss: 20.9206\n",
      "\n",
      "Epoch: 895 |  Train loss: 5.6206 | Val loss: 21.1102\n",
      "\n",
      "Epoch: 896 |  Train loss: 5.2005 | Val loss: 18.1071\n",
      "\n",
      "Epoch: 897 |  Train loss: 5.2668 | Val loss: 18.1046\n",
      "\n",
      "Epoch: 898 |  Train loss: 5.3779 | Val loss: 19.3557\n",
      "\n",
      "Epoch: 899 |  Train loss: 5.4974 | Val loss: 18.7927\n",
      "\n",
      "Epoch: 900 |  Train loss: 5.4590 | Val loss: 20.2844\n",
      "\n",
      "Epoch: 901 |  Train loss: 5.6611 | Val loss: 17.6832\n",
      "\n",
      "Epoch: 902 |  Train loss: 5.2267 | Val loss: 20.3725\n",
      "\n",
      "Epoch: 903 |  Train loss: 5.1608 | Val loss: 19.8546\n",
      "\n",
      "Epoch: 904 |  Train loss: 5.0855 | Val loss: 18.8056\n",
      "\n",
      "Epoch: 905 |  Train loss: 5.0677 | Val loss: 19.8896\n",
      "\n",
      "Epoch: 906 |  Train loss: 5.2566 | Val loss: 19.2819\n",
      "\n",
      "Epoch: 907 |  Train loss: 5.4806 | Val loss: 18.1427\n",
      "\n",
      "Epoch: 908 |  Train loss: 5.4041 | Val loss: 20.9720\n",
      "\n",
      "Epoch: 909 |  Train loss: 5.2434 | Val loss: 18.0428\n",
      "\n",
      "Epoch: 910 |  Train loss: 5.2364 | Val loss: 19.9455\n",
      "\n",
      "Epoch: 911 |  Train loss: 5.1102 | Val loss: 18.2370\n",
      "\n",
      "Epoch: 912 |  Train loss: 5.0916 | Val loss: 19.5641\n",
      "\n",
      "Epoch: 913 |  Train loss: 5.3954 | Val loss: 17.5034\n",
      "\n",
      "Epoch: 914 |  Train loss: 5.1531 | Val loss: 19.4345\n",
      "\n",
      "Epoch: 915 |  Train loss: 5.2050 | Val loss: 18.7308\n",
      "\n",
      "Epoch: 916 |  Train loss: 5.2830 | Val loss: 18.7564\n",
      "\n",
      "Iteration: 11000/12000 | loss = 4.3993\n",
      "Epoch: 917 |  Train loss: 5.1853 | Val loss: 17.7577\n",
      "\n",
      "Epoch: 918 |  Train loss: 5.0851 | Val loss: 18.2281\n",
      "\n",
      "Epoch: 919 |  Train loss: 5.0091 | Val loss: 18.6908\n",
      "\n",
      "Epoch: 920 |  Train loss: 5.0183 | Val loss: 18.9394\n",
      "\n",
      "Epoch: 921 |  Train loss: 5.1705 | Val loss: 19.3904\n",
      "\n",
      "Epoch: 922 |  Train loss: 5.1512 | Val loss: 19.8044\n",
      "\n",
      "Epoch: 923 |  Train loss: 5.2545 | Val loss: 18.3351\n",
      "\n",
      "Epoch: 924 |  Train loss: 5.2870 | Val loss: 19.7186\n",
      "\n",
      "Epoch: 925 |  Train loss: 5.8307 | Val loss: 18.3625\n",
      "\n",
      "Epoch: 926 |  Train loss: 5.4142 | Val loss: 20.1469\n",
      "\n",
      "Epoch: 927 |  Train loss: 5.5642 | Val loss: 16.5927\n",
      "\n",
      "Epoch: 928 |  Train loss: 5.1979 | Val loss: 18.2362\n",
      "\n",
      "Epoch: 929 |  Train loss: 5.1557 | Val loss: 19.0411\n",
      "\n",
      "Epoch: 930 |  Train loss: 5.0373 | Val loss: 18.1692\n",
      "\n",
      "Epoch: 931 |  Train loss: 5.0838 | Val loss: 18.5447\n",
      "\n",
      "Epoch: 932 |  Train loss: 5.1721 | Val loss: 17.7649\n",
      "\n",
      "Epoch: 933 |  Train loss: 5.1873 | Val loss: 20.0911\n",
      "\n",
      "Epoch: 934 |  Train loss: 5.3976 | Val loss: 18.6173\n",
      "\n",
      "Epoch: 935 |  Train loss: 5.8673 | Val loss: 20.9886\n",
      "\n",
      "Epoch: 936 |  Train loss: 5.8582 | Val loss: 17.4504\n",
      "\n",
      "Epoch: 937 |  Train loss: 5.4941 | Val loss: 19.6666\n",
      "\n",
      "Epoch: 938 |  Train loss: 5.1304 | Val loss: 18.6729\n",
      "\n",
      "Epoch: 939 |  Train loss: 5.1730 | Val loss: 19.1572\n",
      "\n",
      "Epoch: 940 |  Train loss: 5.1659 | Val loss: 19.2606\n",
      "\n",
      "Epoch: 941 |  Train loss: 5.0554 | Val loss: 18.8627\n",
      "\n",
      "Epoch: 942 |  Train loss: 5.0293 | Val loss: 19.3573\n",
      "\n",
      "Epoch: 943 |  Train loss: 5.2911 | Val loss: 18.1005\n",
      "\n",
      "Epoch: 944 |  Train loss: 5.0439 | Val loss: 19.9703\n",
      "\n",
      "Epoch: 945 |  Train loss: 5.2500 | Val loss: 19.8838\n",
      "\n",
      "Epoch: 946 |  Train loss: 5.5323 | Val loss: 18.4623\n",
      "\n",
      "Epoch: 947 |  Train loss: 5.6044 | Val loss: 20.3520\n",
      "\n",
      "Epoch: 948 |  Train loss: 6.2663 | Val loss: 20.7747\n",
      "\n",
      "Epoch: 949 |  Train loss: 5.7774 | Val loss: 22.1370\n",
      "\n",
      "Epoch: 950 |  Train loss: 5.4494 | Val loss: 19.3093\n",
      "\n",
      "Epoch: 951 |  Train loss: 5.2998 | Val loss: 19.6194\n",
      "\n",
      "Epoch: 952 |  Train loss: 5.0673 | Val loss: 18.1507\n",
      "\n",
      "Epoch: 953 |  Train loss: 5.0384 | Val loss: 18.5895\n",
      "\n",
      "Epoch: 954 |  Train loss: 5.1043 | Val loss: 19.5233\n",
      "\n",
      "Epoch: 955 |  Train loss: 5.1480 | Val loss: 17.8387\n",
      "\n",
      "Epoch: 956 |  Train loss: 5.2962 | Val loss: 18.7686\n",
      "\n",
      "Epoch: 957 |  Train loss: 5.1498 | Val loss: 18.3232\n",
      "\n",
      "Epoch: 958 |  Train loss: 5.1094 | Val loss: 18.1780\n",
      "\n",
      "Epoch: 959 |  Train loss: 5.1199 | Val loss: 19.8563\n",
      "\n",
      "Epoch: 960 |  Train loss: 5.1325 | Val loss: 18.2022\n",
      "\n",
      "Epoch: 961 |  Train loss: 5.1409 | Val loss: 19.6045\n",
      "\n",
      "Epoch: 962 |  Train loss: 5.3431 | Val loss: 18.5283\n",
      "\n",
      "Epoch: 963 |  Train loss: 5.3602 | Val loss: 20.7198\n",
      "\n",
      "Epoch: 964 |  Train loss: 5.0202 | Val loss: 18.9088\n",
      "\n",
      "Epoch: 965 |  Train loss: 4.9922 | Val loss: 18.8996\n",
      "\n",
      "Epoch: 966 |  Train loss: 5.2280 | Val loss: 19.6557\n",
      "\n",
      "Epoch: 967 |  Train loss: 5.1171 | Val loss: 19.0689\n",
      "\n",
      "Epoch: 968 |  Train loss: 5.0472 | Val loss: 19.0156\n",
      "\n",
      "Epoch: 969 |  Train loss: 4.9964 | Val loss: 18.8932\n",
      "\n",
      "Epoch: 970 |  Train loss: 5.0653 | Val loss: 18.4800\n",
      "\n",
      "Epoch: 971 |  Train loss: 5.0425 | Val loss: 19.1018\n",
      "\n",
      "Epoch: 972 |  Train loss: 5.0604 | Val loss: 19.1339\n",
      "\n",
      "Epoch: 973 |  Train loss: 5.1980 | Val loss: 18.3809\n",
      "\n",
      "Epoch: 974 |  Train loss: 5.0918 | Val loss: 19.5813\n",
      "\n",
      "Epoch: 975 |  Train loss: 5.0449 | Val loss: 18.3523\n",
      "\n",
      "Epoch: 976 |  Train loss: 5.4423 | Val loss: 20.6782\n",
      "\n",
      "Epoch: 977 |  Train loss: 5.3151 | Val loss: 18.3263\n",
      "\n",
      "Epoch: 978 |  Train loss: 5.1824 | Val loss: 20.8549\n",
      "\n",
      "Epoch: 979 |  Train loss: 5.0374 | Val loss: 20.2591\n",
      "\n",
      "Epoch: 980 |  Train loss: 5.2207 | Val loss: 19.0222\n",
      "\n",
      "Epoch: 981 |  Train loss: 5.1643 | Val loss: 17.8880\n",
      "\n",
      "Epoch: 982 |  Train loss: 5.2253 | Val loss: 17.9648\n",
      "\n",
      "Epoch: 983 |  Train loss: 5.0839 | Val loss: 19.3690\n",
      "\n",
      "Epoch: 984 |  Train loss: 5.5091 | Val loss: 18.8643\n",
      "\n",
      "Epoch: 985 |  Train loss: 5.7444 | Val loss: 19.7537\n",
      "\n",
      "Epoch: 986 |  Train loss: 5.2812 | Val loss: 18.2179\n",
      "\n",
      "Epoch: 987 |  Train loss: 5.0218 | Val loss: 19.1831\n",
      "\n",
      "Epoch: 988 |  Train loss: 4.9840 | Val loss: 19.2433\n",
      "\n",
      "Epoch: 989 |  Train loss: 4.9970 | Val loss: 19.3403\n",
      "\n",
      "Epoch: 990 |  Train loss: 5.0038 | Val loss: 18.9466\n",
      "\n",
      "Epoch: 991 |  Train loss: 5.3008 | Val loss: 18.6773\n",
      "\n",
      "Epoch: 992 |  Train loss: 5.3124 | Val loss: 18.0334\n",
      "\n",
      "Epoch: 993 |  Train loss: 5.0279 | Val loss: 19.0167\n",
      "\n",
      "Epoch: 994 |  Train loss: 5.3250 | Val loss: 18.1234\n",
      "\n",
      "Epoch: 995 |  Train loss: 5.0343 | Val loss: 19.0094\n",
      "\n",
      "Epoch: 996 |  Train loss: 5.0607 | Val loss: 17.4682\n",
      "\n",
      "Epoch: 997 |  Train loss: 5.0218 | Val loss: 18.4218\n",
      "\n",
      "Epoch: 998 |  Train loss: 5.0169 | Val loss: 18.0479\n",
      "\n",
      "Epoch: 999 |  Train loss: 5.0439 | Val loss: 19.8590\n",
      "\n",
      "Iteration: 12000/12000 | loss = 1.1657\n",
      "Epoch: 1000 |  Train loss: 4.9481 | Val loss: 18.4036\n",
      "\n",
      "\n",
      "Best Epoch: 816 | Val loss: 16.2655\n",
      "Best Parameters have been loaded in the model\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "param_config = config[\"parameters\"]\n",
    "# getting the model, criterion and optimizer\n",
    "model = get_model(param_config[\"model\"])\n",
    "print(model.parameters().keys())\n",
    "criterion = get_criterion(param_config[\"criterion\"])\n",
    "optimizer = get_optimizer(param_config[\"optimizer\"], model)\n",
    "\n",
    "# training config\n",
    "train_config = param_config[\"training\"]\n",
    "\n",
    "train_loss_history, val_loss_history = train(\n",
    "    model, criterion, optimizer, X_train, y_train, X_val, y_val, **train_config)\n",
    "\n",
    "# best_model = trigger_training(config[\"parameters\"], X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"smai-assignment3-task3\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "wandb.run.name = \"testing\"\n",
    "trigger_training(config[\"parameters\"], X_train, y_train, X_val, y_val)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_agent_manager():\n",
    "    global X_train, y_train, X_val, y_val\n",
    "    run = wandb.init()\n",
    "    config = re_nest_config(dict(wandb.config))\n",
    "    # setting the wandb run name for the current config\n",
    "    run_name = make_wandb_run_name(config)\n",
    "    print(f\"Run Name: {run_name}\")\n",
    "    run.name = run_name\n",
    "    # start the training\n",
    "    trigger_training(config, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid', 'name': 'single-layer-sgd-grid-search', 'parameters': {'criterion': {'value': 'MSE'}, 'model.input_dim': {'value': 13}, 'model.activation': {'values': ['relu', 'sigmoid', 'tanh']}, 'model.hidden_dims1': {'values': [8, 16, 32, 64]}, 'model.num_classes': {'value': 1}, 'optimizer.update_rule': {'values': ['sgd']}, 'optimizer.learning_rate': {'values': [0.0001, 0.0005, 0.001, 0.01, 0.1]}, 'training.update_type': {'values': ['minibatch', 'batch', 'stochastic']}, 'training.batch_size': {'values': [16, 32, 64, 128]}, 'training.epochs': {'value': 500}, 'training.calc_accuracy': {'value': False}, 'training.log_wandb': {'values': [True]}, 'training.verbose': {'value': False}}}\n"
     ]
    }
   ],
   "source": [
    "print(sweep_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid', 'project': 'smai-assignment3-task2', 'name': 'single-layer-grid-search', 'parameters': {'criterion': {'value': 'MSE'}, 'model.input_dim': {'value': 13}, 'model.activation': {'values': ['relu', 'sigmoid', 'tanh']}, 'model.num_layers': {'value': 1}, 'model.hidden_dims1': {'values': [8, 16, 32, 64]}, 'model.num_classes': {'value': 1}, 'optimizer.update_rule': {'values': ['adam', 'sgd']}, 'optimizer.learning_rate': {'values': [0.0001, 0.0005, 0.001, 0.01]}, 'training.update_type': {'values': ['minibatch', 'batch', 'stochastic']}, 'training.batch_size': {'values': [8, 16, 32, 64]}, 'training.epochs': {'value': 500}, 'training.log_wandb': {'value': True}, 'training.calc_accuracy': {'value': False}, 'training.verbose': {'value': False}}}\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.join(YAML_FOLDER, \"single-layer-grid-search.yaml\"), \"r\")\n",
    "\n",
    "sweep_configuration = yaml.safe_load(f)\n",
    "print(sweep_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xnt01pmd\n",
      "Sweep URL: https://wandb.ai/vanshg/smai-assignment3-task3/sweeps/xnt01pmd\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"smai-assignment3-task3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=sweep_agent_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.4) Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
      "<function MSELoss at 0x7f6ade7b70a0>\n",
      "Number of Iterations Per Epoch: 23\n",
      "Epoch: 1 |  Train loss: 36.8907 | Val loss: 48.9935\n",
      "\n",
      "Epoch: 2 |  Train loss: 64.3557 | Val loss: 60.6529\n",
      "\n",
      "Epoch: 3 |  Train loss: 48.8387 | Val loss: 52.1529\n",
      "\n",
      "Epoch: 4 |  Train loss: 31.8695 | Val loss: 41.9256\n",
      "\n",
      "Epoch: 5 |  Train loss: 27.9885 | Val loss: 28.3298\n",
      "\n",
      "Epoch: 6 |  Train loss: 31.8144 | Val loss: 32.7744\n",
      "\n",
      "Epoch: 7 |  Train loss: 19.0079 | Val loss: 30.8631\n",
      "\n",
      "Epoch: 8 |  Train loss: 23.4938 | Val loss: 31.6953\n",
      "\n",
      "Epoch: 9 |  Train loss: 17.7724 | Val loss: 22.9875\n",
      "\n",
      "Epoch: 10 |  Train loss: 13.7641 | Val loss: 19.3578\n",
      "\n",
      "Epoch: 11 |  Train loss: 18.5461 | Val loss: 32.8306\n",
      "\n",
      "Epoch: 12 |  Train loss: 24.2411 | Val loss: 27.3765\n",
      "\n",
      "Epoch: 13 |  Train loss: 49.5855 | Val loss: 77.3675\n",
      "\n",
      "Epoch: 14 |  Train loss: 43.4749 | Val loss: 45.8414\n",
      "\n",
      "Epoch: 15 |  Train loss: 36.0683 | Val loss: 50.0790\n",
      "\n",
      "Epoch: 16 |  Train loss: 10.7064 | Val loss: 19.7287\n",
      "\n",
      "Epoch: 17 |  Train loss: 15.2864 | Val loss: 20.8967\n",
      "\n",
      "Epoch: 18 |  Train loss: 12.8899 | Val loss: 19.2090\n",
      "\n",
      "Epoch: 19 |  Train loss: 12.4596 | Val loss: 19.8144\n",
      "\n",
      "Epoch: 20 |  Train loss: 17.0092 | Val loss: 26.0793\n",
      "\n",
      "Epoch: 21 |  Train loss: 40.0713 | Val loss: 49.7137\n",
      "\n",
      "Epoch: 22 |  Train loss: 12.1003 | Val loss: 23.3293\n",
      "\n",
      "Epoch: 23 |  Train loss: 13.1006 | Val loss: 21.5646\n",
      "\n",
      "Epoch: 24 |  Train loss: 13.0215 | Val loss: 21.5064\n",
      "\n",
      "Epoch: 25 |  Train loss: 19.2848 | Val loss: 28.7879\n",
      "\n",
      "Epoch: 26 |  Train loss: 65.0688 | Val loss: 90.2104\n",
      "\n",
      "Epoch: 27 |  Train loss: 19.8588 | Val loss: 32.0064\n",
      "\n",
      "Epoch: 28 |  Train loss: 12.3248 | Val loss: 21.2907\n",
      "\n",
      "Epoch: 29 |  Train loss: 42.5248 | Val loss: 46.8910\n",
      "\n",
      "Epoch: 30 |  Train loss: 22.6041 | Val loss: 37.4278\n",
      "\n",
      "Epoch: 31 |  Train loss: 29.6712 | Val loss: 39.7490\n",
      "\n",
      "Epoch: 32 |  Train loss: 7.8385 | Val loss: 17.8313\n",
      "\n",
      "Epoch: 33 |  Train loss: 8.2803 | Val loss: 18.8333\n",
      "\n",
      "Epoch: 34 |  Train loss: 16.2435 | Val loss: 30.0845\n",
      "\n",
      "Epoch: 35 |  Train loss: 12.0815 | Val loss: 26.6913\n",
      "\n",
      "Epoch: 36 |  Train loss: 8.6371 | Val loss: 19.6571\n",
      "\n",
      "Epoch: 37 |  Train loss: 96.3920 | Val loss: 99.9822\n",
      "\n",
      "Epoch: 38 |  Train loss: 19.9313 | Val loss: 25.5807\n",
      "\n",
      "Epoch: 39 |  Train loss: 10.3994 | Val loss: 17.8733\n",
      "\n",
      "Epoch: 40 |  Train loss: 73.3936 | Val loss: 53.9968\n",
      "\n",
      "Epoch: 41 |  Train loss: 32.3956 | Val loss: 53.4318\n",
      "\n",
      "Epoch: 42 |  Train loss: 31.5224 | Val loss: 30.3893\n",
      "\n",
      "Epoch: 43 |  Train loss: 9.3365 | Val loss: 19.7022\n",
      "\n",
      "Iteration: 1000/11500 | loss = 8.7867\n",
      "Epoch: 44 |  Train loss: 20.2047 | Val loss: 30.4205\n",
      "\n",
      "Epoch: 45 |  Train loss: 11.2870 | Val loss: 20.6870\n",
      "\n",
      "Epoch: 46 |  Train loss: 7.4376 | Val loss: 16.6566\n",
      "\n",
      "Epoch: 47 |  Train loss: 26.5980 | Val loss: 40.2492\n",
      "\n",
      "Epoch: 48 |  Train loss: 7.7742 | Val loss: 17.0670\n",
      "\n",
      "Epoch: 49 |  Train loss: 8.8426 | Val loss: 18.0343\n",
      "\n",
      "Epoch: 50 |  Train loss: 15.6973 | Val loss: 29.5449\n",
      "\n",
      "Epoch: 51 |  Train loss: 13.6178 | Val loss: 16.7232\n",
      "\n",
      "Epoch: 52 |  Train loss: 7.9752 | Val loss: 16.0879\n",
      "\n",
      "Epoch: 53 |  Train loss: 9.9356 | Val loss: 22.0209\n",
      "\n",
      "Epoch: 54 |  Train loss: 6.8733 | Val loss: 18.0063\n",
      "\n",
      "Epoch: 55 |  Train loss: 21.5192 | Val loss: 25.7733\n",
      "\n",
      "Epoch: 56 |  Train loss: 11.1880 | Val loss: 22.5151\n",
      "\n",
      "Epoch: 57 |  Train loss: 10.9278 | Val loss: 19.0353\n",
      "\n",
      "Epoch: 58 |  Train loss: 14.4403 | Val loss: 24.4660\n",
      "\n",
      "Epoch: 59 |  Train loss: 12.3995 | Val loss: 29.2587\n",
      "\n",
      "Epoch: 60 |  Train loss: 99.9038 | Val loss: 116.6718\n",
      "\n",
      "Epoch: 61 |  Train loss: 35.0843 | Val loss: 50.8059\n",
      "\n",
      "Epoch: 62 |  Train loss: 31.7660 | Val loss: 43.2343\n",
      "\n",
      "Epoch: 63 |  Train loss: 9.0154 | Val loss: 16.7071\n",
      "\n",
      "Epoch: 64 |  Train loss: 8.8328 | Val loss: 17.5113\n",
      "\n",
      "Epoch: 65 |  Train loss: 10.9080 | Val loss: 19.0088\n",
      "\n",
      "Epoch: 66 |  Train loss: 7.3329 | Val loss: 17.3796\n",
      "\n",
      "Epoch: 67 |  Train loss: 15.2823 | Val loss: 27.2042\n",
      "\n",
      "Epoch: 68 |  Train loss: 8.1843 | Val loss: 19.9599\n",
      "\n",
      "Epoch: 69 |  Train loss: 81.9435 | Val loss: 82.8329\n",
      "\n",
      "Epoch: 70 |  Train loss: 11.3116 | Val loss: 17.8422\n",
      "\n",
      "Epoch: 71 |  Train loss: 8.5250 | Val loss: 15.2393\n",
      "\n",
      "Epoch: 72 |  Train loss: 8.1506 | Val loss: 19.6005\n",
      "\n",
      "Epoch: 73 |  Train loss: 13.6092 | Val loss: 25.6168\n",
      "\n",
      "Epoch: 74 |  Train loss: 21.3260 | Val loss: 38.3639\n",
      "\n",
      "Epoch: 75 |  Train loss: 9.8308 | Val loss: 19.0112\n",
      "\n",
      "Epoch: 76 |  Train loss: 20.3439 | Val loss: 21.7753\n",
      "\n",
      "Epoch: 77 |  Train loss: 12.9215 | Val loss: 24.8833\n",
      "\n",
      "Epoch: 78 |  Train loss: 28.6229 | Val loss: 49.7210\n",
      "\n",
      "Epoch: 79 |  Train loss: 7.0550 | Val loss: 19.1384\n",
      "\n",
      "Epoch: 80 |  Train loss: 9.5818 | Val loss: 17.8761\n",
      "\n",
      "Epoch: 81 |  Train loss: 6.3359 | Val loss: 15.6606\n",
      "\n",
      "Epoch: 82 |  Train loss: 8.5900 | Val loss: 15.6981\n",
      "\n",
      "Epoch: 83 |  Train loss: 11.9755 | Val loss: 23.0014\n",
      "\n",
      "Epoch: 84 |  Train loss: 12.9922 | Val loss: 29.8851\n",
      "\n",
      "Epoch: 85 |  Train loss: 20.0363 | Val loss: 36.5882\n",
      "\n",
      "Epoch: 86 |  Train loss: 20.1158 | Val loss: 24.9966\n",
      "\n",
      "Iteration: 2000/11500 | loss = 6.0413\n",
      "Epoch: 87 |  Train loss: 82.0398 | Val loss: 112.4109\n",
      "\n",
      "Epoch: 88 |  Train loss: 12.7797 | Val loss: 21.6104\n",
      "\n",
      "Epoch: 89 |  Train loss: 12.8980 | Val loss: 23.8409\n",
      "\n",
      "Epoch: 90 |  Train loss: 9.6437 | Val loss: 21.2603\n",
      "\n",
      "Epoch: 91 |  Train loss: 32.1842 | Val loss: 54.1259\n",
      "\n",
      "Epoch: 92 |  Train loss: 20.2923 | Val loss: 25.1999\n",
      "\n",
      "Epoch: 93 |  Train loss: 11.1567 | Val loss: 19.8319\n",
      "\n",
      "Epoch: 94 |  Train loss: 14.2260 | Val loss: 25.1256\n",
      "\n",
      "Epoch: 95 |  Train loss: 7.3604 | Val loss: 19.1170\n",
      "\n",
      "Epoch: 96 |  Train loss: 7.0071 | Val loss: 15.4295\n",
      "\n",
      "Epoch: 97 |  Train loss: 11.8548 | Val loss: 24.8131\n",
      "\n",
      "Epoch: 98 |  Train loss: 17.1097 | Val loss: 26.3148\n",
      "\n",
      "Epoch: 99 |  Train loss: 9.4335 | Val loss: 19.1962\n",
      "\n",
      "Epoch: 100 |  Train loss: 16.6684 | Val loss: 23.3821\n",
      "\n",
      "Epoch: 101 |  Train loss: 6.2947 | Val loss: 18.1955\n",
      "\n",
      "Epoch: 102 |  Train loss: 9.4546 | Val loss: 18.1913\n",
      "\n",
      "Epoch: 103 |  Train loss: 9.7544 | Val loss: 18.4491\n",
      "\n",
      "Epoch: 104 |  Train loss: 16.4401 | Val loss: 30.5248\n",
      "\n",
      "Epoch: 105 |  Train loss: 6.1937 | Val loss: 16.3300\n",
      "\n",
      "Epoch: 106 |  Train loss: 17.6533 | Val loss: 19.1802\n",
      "\n",
      "Epoch: 107 |  Train loss: 6.6108 | Val loss: 19.2208\n",
      "\n",
      "Epoch: 108 |  Train loss: 13.9846 | Val loss: 20.5655\n",
      "\n",
      "Epoch: 109 |  Train loss: 6.4991 | Val loss: 18.2775\n",
      "\n",
      "Epoch: 110 |  Train loss: 21.2786 | Val loss: 39.6112\n",
      "\n",
      "Epoch: 111 |  Train loss: 110.3652 | Val loss: 59.5123\n",
      "\n",
      "Epoch: 112 |  Train loss: 6.0025 | Val loss: 17.8341\n",
      "\n",
      "Epoch: 113 |  Train loss: 11.8395 | Val loss: 17.1094\n",
      "\n",
      "Epoch: 114 |  Train loss: 7.1746 | Val loss: 15.7452\n",
      "\n",
      "Epoch: 115 |  Train loss: 108.3197 | Val loss: 111.1268\n",
      "\n",
      "Epoch: 116 |  Train loss: 12.5865 | Val loss: 18.9440\n",
      "\n",
      "Epoch: 117 |  Train loss: 7.0363 | Val loss: 17.7809\n",
      "\n",
      "Epoch: 118 |  Train loss: 6.5355 | Val loss: 20.2641\n",
      "\n",
      "Epoch: 119 |  Train loss: 7.8475 | Val loss: 16.7549\n",
      "\n",
      "Epoch: 120 |  Train loss: 13.1090 | Val loss: 21.0968\n",
      "\n",
      "Epoch: 121 |  Train loss: 9.7858 | Val loss: 24.4275\n",
      "\n",
      "Epoch: 122 |  Train loss: 8.0305 | Val loss: 23.6128\n",
      "\n",
      "Epoch: 123 |  Train loss: 41.9232 | Val loss: 39.7959\n",
      "\n",
      "Epoch: 124 |  Train loss: 7.2412 | Val loss: 19.3846\n",
      "\n",
      "Epoch: 125 |  Train loss: 12.7970 | Val loss: 23.2287\n",
      "\n",
      "Epoch: 126 |  Train loss: 6.5884 | Val loss: 17.6446\n",
      "\n",
      "Epoch: 127 |  Train loss: 5.7522 | Val loss: 15.0602\n",
      "\n",
      "Epoch: 128 |  Train loss: 8.5105 | Val loss: 20.8134\n",
      "\n",
      "Epoch: 129 |  Train loss: 7.1938 | Val loss: 21.1372\n",
      "\n",
      "Epoch: 130 |  Train loss: 20.0235 | Val loss: 22.9592\n",
      "\n",
      "Iteration: 3000/11500 | loss = 5.8983\n",
      "Epoch: 131 |  Train loss: 8.3467 | Val loss: 20.8874\n",
      "\n",
      "Epoch: 132 |  Train loss: 24.8924 | Val loss: 32.2021\n",
      "\n",
      "Epoch: 133 |  Train loss: 18.6228 | Val loss: 24.0844\n",
      "\n",
      "Epoch: 134 |  Train loss: 11.4801 | Val loss: 20.0330\n",
      "\n",
      "Epoch: 135 |  Train loss: 7.3035 | Val loss: 16.7180\n",
      "\n",
      "Epoch: 136 |  Train loss: 9.9129 | Val loss: 24.2082\n",
      "\n",
      "Epoch: 137 |  Train loss: 19.3367 | Val loss: 23.7800\n",
      "\n",
      "Epoch: 138 |  Train loss: 17.4050 | Val loss: 32.8952\n",
      "\n",
      "Epoch: 139 |  Train loss: 6.5116 | Val loss: 19.3933\n",
      "\n",
      "Epoch: 140 |  Train loss: 7.9188 | Val loss: 17.7812\n",
      "\n",
      "Epoch: 141 |  Train loss: 59.6017 | Val loss: 56.4952\n",
      "\n",
      "Epoch: 142 |  Train loss: 5.5447 | Val loss: 18.3444\n",
      "\n",
      "Epoch: 143 |  Train loss: 14.5157 | Val loss: 27.2832\n",
      "\n",
      "Epoch: 144 |  Train loss: 7.7212 | Val loss: 17.6918\n",
      "\n",
      "Epoch: 145 |  Train loss: 19.9519 | Val loss: 24.9056\n",
      "\n",
      "Epoch: 146 |  Train loss: 30.8273 | Val loss: 56.0930\n",
      "\n",
      "Epoch: 147 |  Train loss: 16.1418 | Val loss: 24.2790\n",
      "\n",
      "Epoch: 148 |  Train loss: 8.0628 | Val loss: 17.7934\n",
      "\n",
      "Epoch: 149 |  Train loss: 13.7578 | Val loss: 29.3250\n",
      "\n",
      "Epoch: 150 |  Train loss: 5.6857 | Val loss: 19.9720\n",
      "\n",
      "Epoch: 151 |  Train loss: 6.9455 | Val loss: 16.1149\n",
      "\n",
      "Epoch: 152 |  Train loss: 6.4917 | Val loss: 15.6414\n",
      "\n",
      "Epoch: 153 |  Train loss: 5.5129 | Val loss: 15.9775\n",
      "\n",
      "Epoch: 154 |  Train loss: 8.0306 | Val loss: 22.3377\n",
      "\n",
      "Epoch: 155 |  Train loss: 12.4384 | Val loss: 24.9880\n",
      "\n",
      "Epoch: 156 |  Train loss: 7.1273 | Val loss: 17.7281\n",
      "\n",
      "Epoch: 157 |  Train loss: 14.4766 | Val loss: 22.3984\n",
      "\n",
      "Epoch: 158 |  Train loss: 21.2946 | Val loss: 43.5244\n",
      "\n",
      "Epoch: 159 |  Train loss: 7.7080 | Val loss: 20.2916\n",
      "\n",
      "Epoch: 160 |  Train loss: 13.8262 | Val loss: 16.1056\n",
      "\n",
      "Epoch: 161 |  Train loss: 13.0111 | Val loss: 32.5099\n",
      "\n",
      "Epoch: 162 |  Train loss: 5.0662 | Val loss: 17.7551\n",
      "\n",
      "Epoch: 163 |  Train loss: 6.3499 | Val loss: 18.6863\n",
      "\n",
      "Epoch: 164 |  Train loss: 8.1704 | Val loss: 24.6287\n",
      "\n",
      "Epoch: 165 |  Train loss: 4.6637 | Val loss: 19.2903\n",
      "\n",
      "Epoch: 166 |  Train loss: 4.7202 | Val loss: 16.8333\n",
      "\n",
      "Epoch: 167 |  Train loss: 25.6355 | Val loss: 40.6959\n",
      "\n",
      "Epoch: 168 |  Train loss: 12.2883 | Val loss: 20.5914\n",
      "\n",
      "Epoch: 169 |  Train loss: 5.2206 | Val loss: 16.3734\n",
      "\n",
      "Epoch: 170 |  Train loss: 8.3429 | Val loss: 21.9544\n",
      "\n",
      "Epoch: 171 |  Train loss: 6.4752 | Val loss: 21.7761\n",
      "\n",
      "Epoch: 172 |  Train loss: 7.1533 | Val loss: 17.9996\n",
      "\n",
      "Epoch: 173 |  Train loss: 5.3242 | Val loss: 19.6899\n",
      "\n",
      "Iteration: 4000/11500 | loss = 5.1064\n",
      "Epoch: 174 |  Train loss: 4.9742 | Val loss: 16.3700\n",
      "\n",
      "Epoch: 175 |  Train loss: 6.1257 | Val loss: 18.6301\n",
      "\n",
      "Epoch: 176 |  Train loss: 9.3731 | Val loss: 20.3058\n",
      "\n",
      "Epoch: 177 |  Train loss: 6.9603 | Val loss: 16.9448\n",
      "\n",
      "Epoch: 178 |  Train loss: 5.7773 | Val loss: 16.2371\n",
      "\n",
      "Epoch: 179 |  Train loss: 6.2433 | Val loss: 16.6530\n",
      "\n",
      "Epoch: 180 |  Train loss: 7.6066 | Val loss: 22.6565\n",
      "\n",
      "Epoch: 181 |  Train loss: 15.8330 | Val loss: 24.7085\n",
      "\n",
      "Epoch: 182 |  Train loss: 5.2368 | Val loss: 19.5701\n",
      "\n",
      "Epoch: 183 |  Train loss: 13.8881 | Val loss: 22.6362\n",
      "\n",
      "Epoch: 184 |  Train loss: 17.0971 | Val loss: 21.0869\n",
      "\n",
      "Epoch: 185 |  Train loss: 5.6975 | Val loss: 18.1286\n",
      "\n",
      "Epoch: 186 |  Train loss: 15.7807 | Val loss: 24.4012\n",
      "\n",
      "Epoch: 187 |  Train loss: 8.0421 | Val loss: 21.5058\n",
      "\n",
      "Epoch: 188 |  Train loss: 109.5202 | Val loss: 149.3439\n",
      "\n",
      "Epoch: 189 |  Train loss: 11.6976 | Val loss: 21.6219\n",
      "\n",
      "Epoch: 190 |  Train loss: 10.5004 | Val loss: 17.4458\n",
      "\n",
      "Epoch: 191 |  Train loss: 5.5585 | Val loss: 18.0207\n",
      "\n",
      "Epoch: 192 |  Train loss: 5.1361 | Val loss: 16.5350\n",
      "\n",
      "Epoch: 193 |  Train loss: 9.5074 | Val loss: 13.4602\n",
      "\n",
      "Epoch: 194 |  Train loss: 5.3505 | Val loss: 17.6656\n",
      "\n",
      "Epoch: 195 |  Train loss: 5.5107 | Val loss: 15.1868\n",
      "\n",
      "Epoch: 196 |  Train loss: 13.8965 | Val loss: 30.7898\n",
      "\n",
      "Epoch: 197 |  Train loss: 7.5723 | Val loss: 17.4678\n",
      "\n",
      "Epoch: 198 |  Train loss: 8.9047 | Val loss: 22.9851\n",
      "\n",
      "Epoch: 199 |  Train loss: 5.5689 | Val loss: 15.4361\n",
      "\n",
      "Epoch: 200 |  Train loss: 12.2479 | Val loss: 24.4797\n",
      "\n",
      "Epoch: 201 |  Train loss: 4.6959 | Val loss: 15.9271\n",
      "\n",
      "Epoch: 202 |  Train loss: 9.1669 | Val loss: 18.9264\n",
      "\n",
      "Epoch: 203 |  Train loss: 6.1152 | Val loss: 15.2642\n",
      "\n",
      "Epoch: 204 |  Train loss: 17.7220 | Val loss: 36.1016\n",
      "\n",
      "Epoch: 205 |  Train loss: 22.6548 | Val loss: 30.5699\n",
      "\n",
      "Epoch: 206 |  Train loss: 13.6249 | Val loss: 18.9688\n",
      "\n",
      "Epoch: 207 |  Train loss: 5.3480 | Val loss: 15.4621\n",
      "\n",
      "Epoch: 208 |  Train loss: 5.0714 | Val loss: 17.9460\n",
      "\n",
      "Epoch: 209 |  Train loss: 5.8320 | Val loss: 17.9570\n",
      "\n",
      "Epoch: 210 |  Train loss: 6.1167 | Val loss: 17.8450\n",
      "\n",
      "Epoch: 211 |  Train loss: 18.5294 | Val loss: 25.8828\n",
      "\n",
      "Epoch: 212 |  Train loss: 8.1832 | Val loss: 16.7923\n",
      "\n",
      "Epoch: 213 |  Train loss: 11.4029 | Val loss: 26.6799\n",
      "\n",
      "Epoch: 214 |  Train loss: 18.6377 | Val loss: 39.5144\n",
      "\n",
      "Epoch: 215 |  Train loss: 25.9911 | Val loss: 38.1388\n",
      "\n",
      "Epoch: 216 |  Train loss: 5.3118 | Val loss: 16.1281\n",
      "\n",
      "Epoch: 217 |  Train loss: 7.2913 | Val loss: 16.7155\n",
      "\n",
      "Iteration: 5000/11500 | loss = 2.3911\n",
      "Epoch: 218 |  Train loss: 7.9879 | Val loss: 19.2143\n",
      "\n",
      "Epoch: 219 |  Train loss: 6.2102 | Val loss: 20.0114\n",
      "\n",
      "Epoch: 220 |  Train loss: 6.7165 | Val loss: 17.6847\n",
      "\n",
      "Epoch: 221 |  Train loss: 24.1091 | Val loss: 39.8700\n",
      "\n",
      "Epoch: 222 |  Train loss: 8.5715 | Val loss: 21.1825\n",
      "\n",
      "Epoch: 223 |  Train loss: 29.3852 | Val loss: 45.3862\n",
      "\n",
      "Epoch: 224 |  Train loss: 5.8131 | Val loss: 14.6155\n",
      "\n",
      "Epoch: 225 |  Train loss: 20.1735 | Val loss: 15.7475\n",
      "\n",
      "Epoch: 226 |  Train loss: 5.6873 | Val loss: 19.8809\n",
      "\n",
      "Epoch: 227 |  Train loss: 8.1606 | Val loss: 16.5340\n",
      "\n",
      "Epoch: 228 |  Train loss: 9.8962 | Val loss: 16.0841\n",
      "\n",
      "Epoch: 229 |  Train loss: 4.7351 | Val loss: 14.7201\n",
      "\n",
      "Epoch: 230 |  Train loss: 12.7254 | Val loss: 31.4621\n",
      "\n",
      "Epoch: 231 |  Train loss: 8.1754 | Val loss: 19.0875\n",
      "\n",
      "Epoch: 232 |  Train loss: 12.0768 | Val loss: 16.7261\n",
      "\n",
      "Epoch: 233 |  Train loss: 12.0808 | Val loss: 30.1265\n",
      "\n",
      "Epoch: 234 |  Train loss: 10.1182 | Val loss: 27.2349\n",
      "\n",
      "Epoch: 235 |  Train loss: 5.3890 | Val loss: 17.3874\n",
      "\n",
      "Epoch: 236 |  Train loss: 10.3019 | Val loss: 25.5716\n",
      "\n",
      "Epoch: 237 |  Train loss: 25.8886 | Val loss: 22.1313\n",
      "\n",
      "Epoch: 238 |  Train loss: 11.4749 | Val loss: 21.1972\n",
      "\n",
      "Epoch: 239 |  Train loss: 12.9293 | Val loss: 22.4426\n",
      "\n",
      "Epoch: 240 |  Train loss: 5.2931 | Val loss: 16.7280\n",
      "\n",
      "Epoch: 241 |  Train loss: 7.0569 | Val loss: 20.6200\n",
      "\n",
      "Epoch: 242 |  Train loss: 7.2053 | Val loss: 18.8709\n",
      "\n",
      "Epoch: 243 |  Train loss: 5.4714 | Val loss: 15.1809\n",
      "\n",
      "Epoch: 244 |  Train loss: 11.0174 | Val loss: 21.0759\n",
      "\n",
      "Epoch: 245 |  Train loss: 13.8653 | Val loss: 24.3872\n",
      "\n",
      "Epoch: 246 |  Train loss: 7.1026 | Val loss: 18.4015\n",
      "\n",
      "Epoch: 247 |  Train loss: 5.3951 | Val loss: 17.0467\n",
      "\n",
      "Epoch: 248 |  Train loss: 15.7204 | Val loss: 26.9535\n",
      "\n",
      "Epoch: 249 |  Train loss: 33.0199 | Val loss: 46.5912\n",
      "\n",
      "Epoch: 250 |  Train loss: 4.4618 | Val loss: 17.4469\n",
      "\n",
      "Epoch: 251 |  Train loss: 6.4746 | Val loss: 19.5192\n",
      "\n",
      "Epoch: 252 |  Train loss: 4.2704 | Val loss: 17.5496\n",
      "\n",
      "Epoch: 253 |  Train loss: 19.0854 | Val loss: 35.1465\n",
      "\n",
      "Epoch: 254 |  Train loss: 12.5415 | Val loss: 19.2357\n",
      "\n",
      "Epoch: 255 |  Train loss: 7.0507 | Val loss: 20.9347\n",
      "\n",
      "Epoch: 256 |  Train loss: 6.4262 | Val loss: 19.8832\n",
      "\n",
      "Epoch: 257 |  Train loss: 5.9167 | Val loss: 15.3365\n",
      "\n",
      "Epoch: 258 |  Train loss: 4.4959 | Val loss: 16.8575\n",
      "\n",
      "Epoch: 259 |  Train loss: 7.5000 | Val loss: 25.0810\n",
      "\n",
      "Epoch: 260 |  Train loss: 10.6281 | Val loss: 25.9398\n",
      "\n",
      "Iteration: 6000/11500 | loss = 2.4961\n",
      "Epoch: 261 |  Train loss: 4.5594 | Val loss: 15.8286\n",
      "\n",
      "Epoch: 262 |  Train loss: 17.2916 | Val loss: 18.6864\n",
      "\n",
      "Epoch: 263 |  Train loss: 19.9947 | Val loss: 34.8981\n",
      "\n",
      "Epoch: 264 |  Train loss: 6.0427 | Val loss: 15.8274\n",
      "\n",
      "Epoch: 265 |  Train loss: 7.0570 | Val loss: 20.8360\n",
      "\n",
      "Epoch: 266 |  Train loss: 5.4073 | Val loss: 20.3164\n",
      "\n",
      "Epoch: 267 |  Train loss: 20.7524 | Val loss: 29.7482\n",
      "\n",
      "Epoch: 268 |  Train loss: 5.9866 | Val loss: 15.7318\n",
      "\n",
      "Epoch: 269 |  Train loss: 80.6425 | Val loss: 85.9790\n",
      "\n",
      "Epoch: 270 |  Train loss: 9.0195 | Val loss: 27.0236\n",
      "\n",
      "Epoch: 271 |  Train loss: 9.0258 | Val loss: 22.4412\n",
      "\n",
      "Epoch: 272 |  Train loss: 5.0261 | Val loss: 20.3649\n",
      "\n",
      "Epoch: 273 |  Train loss: 5.2518 | Val loss: 18.7967\n",
      "\n",
      "Epoch: 274 |  Train loss: 9.4621 | Val loss: 23.8724\n",
      "\n",
      "Epoch: 275 |  Train loss: 6.5943 | Val loss: 20.0480\n",
      "\n",
      "Epoch: 276 |  Train loss: 5.5272 | Val loss: 20.1749\n",
      "\n",
      "Epoch: 277 |  Train loss: 5.0994 | Val loss: 15.4062\n",
      "\n",
      "Epoch: 278 |  Train loss: 33.6615 | Val loss: 51.8483\n",
      "\n",
      "Epoch: 279 |  Train loss: 4.2145 | Val loss: 16.9530\n",
      "\n",
      "Epoch: 280 |  Train loss: 13.8278 | Val loss: 21.3752\n",
      "\n",
      "Epoch: 281 |  Train loss: 5.0951 | Val loss: 19.1938\n",
      "\n",
      "Epoch: 282 |  Train loss: 4.9188 | Val loss: 18.2411\n",
      "\n",
      "Epoch: 283 |  Train loss: 12.6684 | Val loss: 32.0775\n",
      "\n",
      "Epoch: 284 |  Train loss: 12.1202 | Val loss: 16.1104\n",
      "\n",
      "Epoch: 285 |  Train loss: 5.1319 | Val loss: 16.7631\n",
      "\n",
      "Epoch: 286 |  Train loss: 7.6725 | Val loss: 17.7169\n",
      "\n",
      "Epoch: 287 |  Train loss: 62.1080 | Val loss: 82.2383\n",
      "\n",
      "Epoch: 288 |  Train loss: 6.2606 | Val loss: 18.2987\n",
      "\n",
      "Epoch: 289 |  Train loss: 4.2119 | Val loss: 17.3872\n",
      "\n",
      "Epoch: 290 |  Train loss: 6.5123 | Val loss: 21.3354\n",
      "\n",
      "Epoch: 291 |  Train loss: 4.6982 | Val loss: 17.3587\n",
      "\n",
      "Epoch: 292 |  Train loss: 4.0924 | Val loss: 17.4890\n",
      "\n",
      "Epoch: 293 |  Train loss: 4.0872 | Val loss: 16.2253\n",
      "\n",
      "Epoch: 294 |  Train loss: 12.8361 | Val loss: 20.1629\n",
      "\n",
      "Epoch: 295 |  Train loss: 5.0906 | Val loss: 18.2924\n",
      "\n",
      "Epoch: 296 |  Train loss: 7.2626 | Val loss: 23.9244\n",
      "\n",
      "Epoch: 297 |  Train loss: 7.8402 | Val loss: 22.9528\n",
      "\n",
      "Epoch: 298 |  Train loss: 5.1250 | Val loss: 19.5140\n",
      "\n",
      "Epoch: 299 |  Train loss: 17.7281 | Val loss: 33.6535\n",
      "\n",
      "Epoch: 300 |  Train loss: 8.5885 | Val loss: 24.1645\n",
      "\n",
      "Epoch: 301 |  Train loss: 4.3787 | Val loss: 15.8894\n",
      "\n",
      "Epoch: 302 |  Train loss: 26.2386 | Val loss: 36.4164\n",
      "\n",
      "Epoch: 303 |  Train loss: 26.0966 | Val loss: 41.7887\n",
      "\n",
      "Epoch: 304 |  Train loss: 5.1782 | Val loss: 16.6117\n",
      "\n",
      "Iteration: 7000/11500 | loss = 5.3100\n",
      "Epoch: 305 |  Train loss: 9.3668 | Val loss: 20.1461\n",
      "\n",
      "Epoch: 306 |  Train loss: 5.7301 | Val loss: 17.9115\n",
      "\n",
      "Epoch: 307 |  Train loss: 5.6918 | Val loss: 16.6907\n",
      "\n",
      "Epoch: 308 |  Train loss: 12.5992 | Val loss: 25.0854\n",
      "\n",
      "Epoch: 309 |  Train loss: 11.6694 | Val loss: 18.7398\n",
      "\n",
      "Epoch: 310 |  Train loss: 12.4438 | Val loss: 27.7761\n",
      "\n",
      "Epoch: 311 |  Train loss: 4.1775 | Val loss: 15.5925\n",
      "\n",
      "Epoch: 312 |  Train loss: 11.9791 | Val loss: 22.5081\n",
      "\n",
      "Epoch: 313 |  Train loss: 9.5144 | Val loss: 17.9019\n",
      "\n",
      "Epoch: 314 |  Train loss: 5.1282 | Val loss: 15.7287\n",
      "\n",
      "Epoch: 315 |  Train loss: 20.9304 | Val loss: 44.3583\n",
      "\n",
      "Epoch: 316 |  Train loss: 5.0259 | Val loss: 15.4409\n",
      "\n",
      "Epoch: 317 |  Train loss: 6.4244 | Val loss: 16.7787\n",
      "\n",
      "Epoch: 318 |  Train loss: 14.9851 | Val loss: 28.3249\n",
      "\n",
      "Epoch: 319 |  Train loss: 4.3278 | Val loss: 17.5788\n",
      "\n",
      "Epoch: 320 |  Train loss: 10.0824 | Val loss: 17.6799\n",
      "\n",
      "Epoch: 321 |  Train loss: 10.3448 | Val loss: 24.2461\n",
      "\n",
      "Epoch: 322 |  Train loss: 5.8116 | Val loss: 16.2910\n",
      "\n",
      "Epoch: 323 |  Train loss: 5.0967 | Val loss: 19.3117\n",
      "\n",
      "Epoch: 324 |  Train loss: 13.4513 | Val loss: 37.4295\n",
      "\n",
      "Epoch: 325 |  Train loss: 7.4040 | Val loss: 24.9989\n",
      "\n",
      "Epoch: 326 |  Train loss: 16.8217 | Val loss: 20.1856\n",
      "\n",
      "Epoch: 327 |  Train loss: 5.1253 | Val loss: 21.0137\n",
      "\n",
      "Epoch: 328 |  Train loss: 5.2460 | Val loss: 18.2774\n",
      "\n",
      "Epoch: 329 |  Train loss: 10.7799 | Val loss: 23.7740\n",
      "\n",
      "Epoch: 330 |  Train loss: 9.0589 | Val loss: 20.1177\n",
      "\n",
      "Epoch: 331 |  Train loss: 7.0713 | Val loss: 23.4897\n",
      "\n",
      "Epoch: 332 |  Train loss: 6.2818 | Val loss: 23.1414\n",
      "\n",
      "Epoch: 333 |  Train loss: 20.6817 | Val loss: 30.1502\n",
      "\n",
      "Epoch: 334 |  Train loss: 6.0665 | Val loss: 18.1936\n",
      "\n",
      "Epoch: 335 |  Train loss: 8.3311 | Val loss: 24.5995\n",
      "\n",
      "Epoch: 336 |  Train loss: 8.5019 | Val loss: 21.1937\n",
      "\n",
      "Epoch: 337 |  Train loss: 9.7100 | Val loss: 25.5864\n",
      "\n",
      "Epoch: 338 |  Train loss: 10.6779 | Val loss: 26.3258\n",
      "\n",
      "Epoch: 339 |  Train loss: 4.0258 | Val loss: 17.3390\n",
      "\n",
      "Epoch: 340 |  Train loss: 4.5800 | Val loss: 18.5099\n",
      "\n",
      "Epoch: 341 |  Train loss: 7.0755 | Val loss: 17.8847\n",
      "\n",
      "Epoch: 342 |  Train loss: 5.5761 | Val loss: 19.6240\n",
      "\n",
      "Epoch: 343 |  Train loss: 8.0021 | Val loss: 27.8842\n",
      "\n",
      "Epoch: 344 |  Train loss: 30.4626 | Val loss: 61.1457\n",
      "\n",
      "Epoch: 345 |  Train loss: 6.3722 | Val loss: 22.8011\n",
      "\n",
      "Epoch: 346 |  Train loss: 4.0097 | Val loss: 17.5998\n",
      "\n",
      "Epoch: 347 |  Train loss: 14.2036 | Val loss: 28.3126\n",
      "\n",
      "Iteration: 8000/11500 | loss = 3.0016\n",
      "Epoch: 348 |  Train loss: 4.7879 | Val loss: 19.8150\n",
      "\n",
      "Epoch: 349 |  Train loss: 6.3982 | Val loss: 19.1285\n",
      "\n",
      "Epoch: 350 |  Train loss: 12.2263 | Val loss: 27.2789\n",
      "\n",
      "Epoch: 351 |  Train loss: 10.7371 | Val loss: 25.0056\n",
      "\n",
      "Epoch: 352 |  Train loss: 44.5401 | Val loss: 29.7131\n",
      "\n",
      "Epoch: 353 |  Train loss: 28.8844 | Val loss: 36.3792\n",
      "\n",
      "Epoch: 354 |  Train loss: 8.8767 | Val loss: 23.4943\n",
      "\n",
      "Epoch: 355 |  Train loss: 7.3024 | Val loss: 19.8803\n",
      "\n",
      "Epoch: 356 |  Train loss: 6.0696 | Val loss: 19.9163\n",
      "\n",
      "Epoch: 357 |  Train loss: 5.5977 | Val loss: 18.4739\n",
      "\n",
      "Epoch: 358 |  Train loss: 8.1351 | Val loss: 20.5669\n",
      "\n",
      "Epoch: 359 |  Train loss: 5.0983 | Val loss: 17.0008\n",
      "\n",
      "Epoch: 360 |  Train loss: 12.8425 | Val loss: 23.0796\n",
      "\n",
      "Epoch: 361 |  Train loss: 13.5906 | Val loss: 25.8448\n",
      "\n",
      "Epoch: 362 |  Train loss: 5.7547 | Val loss: 20.6291\n",
      "\n",
      "Epoch: 363 |  Train loss: 7.2560 | Val loss: 17.2980\n",
      "\n",
      "Epoch: 364 |  Train loss: 7.7431 | Val loss: 21.2394\n",
      "\n",
      "Epoch: 365 |  Train loss: 3.8413 | Val loss: 18.6802\n",
      "\n",
      "Epoch: 366 |  Train loss: 4.4640 | Val loss: 17.3584\n",
      "\n",
      "Epoch: 367 |  Train loss: 5.7484 | Val loss: 18.0773\n",
      "\n",
      "Epoch: 368 |  Train loss: 4.3294 | Val loss: 19.1660\n",
      "\n",
      "Epoch: 369 |  Train loss: 18.3694 | Val loss: 36.4913\n",
      "\n",
      "Epoch: 370 |  Train loss: 31.8416 | Val loss: 47.2207\n",
      "\n",
      "Epoch: 371 |  Train loss: 5.4581 | Val loss: 18.0464\n",
      "\n",
      "Epoch: 372 |  Train loss: 4.1947 | Val loss: 19.1497\n",
      "\n",
      "Epoch: 373 |  Train loss: 16.4245 | Val loss: 24.8477\n",
      "\n",
      "Epoch: 374 |  Train loss: 4.9308 | Val loss: 18.5024\n",
      "\n",
      "Epoch: 375 |  Train loss: 7.8079 | Val loss: 18.4442\n",
      "\n",
      "Epoch: 376 |  Train loss: 8.3021 | Val loss: 27.7337\n",
      "\n",
      "Epoch: 377 |  Train loss: 10.5417 | Val loss: 32.1332\n",
      "\n",
      "Epoch: 378 |  Train loss: 8.6457 | Val loss: 24.2432\n",
      "\n",
      "Epoch: 379 |  Train loss: 11.1529 | Val loss: 22.7250\n",
      "\n",
      "Epoch: 380 |  Train loss: 6.0037 | Val loss: 23.6825\n",
      "\n",
      "Epoch: 381 |  Train loss: 5.3454 | Val loss: 20.9289\n",
      "\n",
      "Epoch: 382 |  Train loss: 7.4183 | Val loss: 24.0980\n",
      "\n",
      "Epoch: 383 |  Train loss: 6.0948 | Val loss: 19.1788\n",
      "\n",
      "Epoch: 384 |  Train loss: 10.9341 | Val loss: 21.1297\n",
      "\n",
      "Epoch: 385 |  Train loss: 7.1999 | Val loss: 24.9393\n",
      "\n",
      "Epoch: 386 |  Train loss: 36.2868 | Val loss: 52.8138\n",
      "\n",
      "Epoch: 387 |  Train loss: 6.5390 | Val loss: 22.5825\n",
      "\n",
      "Epoch: 388 |  Train loss: 4.6093 | Val loss: 20.4668\n",
      "\n",
      "Epoch: 389 |  Train loss: 7.9185 | Val loss: 18.9249\n",
      "\n",
      "Epoch: 390 |  Train loss: 4.9850 | Val loss: 20.1800\n",
      "\n",
      "Epoch: 391 |  Train loss: 10.1226 | Val loss: 24.6466\n",
      "\n",
      "Iteration: 9000/11500 | loss = 2.9016\n",
      "Epoch: 392 |  Train loss: 12.2762 | Val loss: 27.6245\n",
      "\n",
      "Epoch: 393 |  Train loss: 5.0957 | Val loss: 19.6482\n",
      "\n",
      "Epoch: 394 |  Train loss: 10.4157 | Val loss: 23.5969\n",
      "\n",
      "Epoch: 395 |  Train loss: 11.9569 | Val loss: 22.0167\n",
      "\n",
      "Epoch: 396 |  Train loss: 3.9553 | Val loss: 18.3690\n",
      "\n",
      "Epoch: 397 |  Train loss: 5.4294 | Val loss: 19.8378\n",
      "\n",
      "Epoch: 398 |  Train loss: 11.1507 | Val loss: 22.3041\n",
      "\n",
      "Epoch: 399 |  Train loss: 6.4005 | Val loss: 19.5361\n",
      "\n",
      "Epoch: 400 |  Train loss: 66.4697 | Val loss: 48.0731\n",
      "\n",
      "Epoch: 401 |  Train loss: 5.2401 | Val loss: 21.8111\n",
      "\n",
      "Epoch: 402 |  Train loss: 18.3469 | Val loss: 28.1307\n",
      "\n",
      "Epoch: 403 |  Train loss: 10.4933 | Val loss: 26.2861\n",
      "\n",
      "Epoch: 404 |  Train loss: 3.9183 | Val loss: 19.3300\n",
      "\n",
      "Epoch: 405 |  Train loss: 13.5676 | Val loss: 25.5281\n",
      "\n",
      "Epoch: 406 |  Train loss: 7.1032 | Val loss: 25.9197\n",
      "\n",
      "Epoch: 407 |  Train loss: 3.9887 | Val loss: 19.8813\n",
      "\n",
      "Epoch: 408 |  Train loss: 11.4086 | Val loss: 19.1722\n",
      "\n",
      "Epoch: 409 |  Train loss: 13.0678 | Val loss: 29.6914\n",
      "\n",
      "Epoch: 410 |  Train loss: 4.0462 | Val loss: 19.9225\n",
      "\n",
      "Epoch: 411 |  Train loss: 19.9425 | Val loss: 22.4221\n",
      "\n",
      "Epoch: 412 |  Train loss: 4.2653 | Val loss: 19.5072\n",
      "\n",
      "Epoch: 413 |  Train loss: 8.2604 | Val loss: 20.0400\n",
      "\n",
      "Epoch: 414 |  Train loss: 5.0748 | Val loss: 19.9879\n",
      "\n",
      "Epoch: 415 |  Train loss: 3.9974 | Val loss: 18.5812\n",
      "\n",
      "Epoch: 416 |  Train loss: 7.0450 | Val loss: 22.5318\n",
      "\n",
      "Epoch: 417 |  Train loss: 6.5348 | Val loss: 19.3843\n",
      "\n",
      "Epoch: 418 |  Train loss: 9.9016 | Val loss: 22.7924\n",
      "\n",
      "Epoch: 419 |  Train loss: 10.4534 | Val loss: 30.5343\n",
      "\n",
      "Epoch: 420 |  Train loss: 13.2783 | Val loss: 31.8069\n",
      "\n",
      "Epoch: 421 |  Train loss: 6.0442 | Val loss: 19.1065\n",
      "\n",
      "Epoch: 422 |  Train loss: 11.6471 | Val loss: 32.0326\n",
      "\n",
      "Epoch: 423 |  Train loss: 7.0909 | Val loss: 16.9504\n",
      "\n",
      "Epoch: 424 |  Train loss: 6.1336 | Val loss: 21.9683\n",
      "\n",
      "Epoch: 425 |  Train loss: 4.4593 | Val loss: 21.3582\n",
      "\n",
      "Epoch: 426 |  Train loss: 4.3619 | Val loss: 20.7048\n",
      "\n",
      "Epoch: 427 |  Train loss: 4.4620 | Val loss: 21.4975\n",
      "\n",
      "Epoch: 428 |  Train loss: 6.2693 | Val loss: 23.2203\n",
      "\n",
      "Epoch: 429 |  Train loss: 16.2490 | Val loss: 26.8548\n",
      "\n",
      "Epoch: 430 |  Train loss: 4.6212 | Val loss: 20.9107\n",
      "\n",
      "Epoch: 431 |  Train loss: 4.9055 | Val loss: 21.2107\n",
      "\n",
      "Epoch: 432 |  Train loss: 21.1986 | Val loss: 44.2276\n",
      "\n",
      "Epoch: 433 |  Train loss: 19.9922 | Val loss: 47.7834\n",
      "\n",
      "Epoch: 434 |  Train loss: 4.4870 | Val loss: 20.5636\n",
      "\n",
      "Iteration: 10000/11500 | loss = 1.9978\n",
      "Epoch: 435 |  Train loss: 7.3207 | Val loss: 25.5756\n",
      "\n",
      "Epoch: 436 |  Train loss: 3.9842 | Val loss: 19.6241\n",
      "\n",
      "Epoch: 437 |  Train loss: 11.3780 | Val loss: 26.1091\n",
      "\n",
      "Epoch: 438 |  Train loss: 6.7707 | Val loss: 27.6605\n",
      "\n",
      "Epoch: 439 |  Train loss: 8.8698 | Val loss: 21.1031\n",
      "\n",
      "Epoch: 440 |  Train loss: 5.2938 | Val loss: 20.9247\n",
      "\n",
      "Epoch: 441 |  Train loss: 9.2928 | Val loss: 25.7243\n",
      "\n",
      "Epoch: 442 |  Train loss: 26.9418 | Val loss: 41.7321\n",
      "\n",
      "Epoch: 443 |  Train loss: 35.6597 | Val loss: 44.5422\n",
      "\n",
      "Epoch: 444 |  Train loss: 8.6655 | Val loss: 24.6444\n",
      "\n",
      "Epoch: 445 |  Train loss: 5.6853 | Val loss: 20.3666\n",
      "\n",
      "Epoch: 446 |  Train loss: 3.6781 | Val loss: 18.5109\n",
      "\n",
      "Epoch: 447 |  Train loss: 79.7197 | Val loss: 83.6472\n",
      "\n",
      "Epoch: 448 |  Train loss: 9.7217 | Val loss: 29.5528\n",
      "\n",
      "Epoch: 449 |  Train loss: 19.1190 | Val loss: 34.6288\n",
      "\n",
      "Epoch: 450 |  Train loss: 5.7465 | Val loss: 22.7212\n",
      "\n",
      "Epoch: 451 |  Train loss: 3.9195 | Val loss: 18.4225\n",
      "\n",
      "Epoch: 452 |  Train loss: 5.1734 | Val loss: 24.1110\n",
      "\n",
      "Epoch: 453 |  Train loss: 9.7876 | Val loss: 24.0042\n",
      "\n",
      "Epoch: 454 |  Train loss: 17.7348 | Val loss: 29.1593\n",
      "\n",
      "Epoch: 455 |  Train loss: 6.6940 | Val loss: 25.9057\n",
      "\n",
      "Epoch: 456 |  Train loss: 15.5485 | Val loss: 38.3501\n",
      "\n",
      "Epoch: 457 |  Train loss: 10.9015 | Val loss: 22.7377\n",
      "\n",
      "Epoch: 458 |  Train loss: 6.0790 | Val loss: 22.9155\n",
      "\n",
      "Epoch: 459 |  Train loss: 8.4323 | Val loss: 28.0578\n",
      "\n",
      "Epoch: 460 |  Train loss: 12.4478 | Val loss: 25.7473\n",
      "\n",
      "Epoch: 461 |  Train loss: 3.8912 | Val loss: 21.5395\n",
      "\n",
      "Epoch: 462 |  Train loss: 10.7356 | Val loss: 28.4983\n",
      "\n",
      "Epoch: 463 |  Train loss: 8.6008 | Val loss: 29.0593\n",
      "\n",
      "Epoch: 464 |  Train loss: 5.4163 | Val loss: 20.0930\n",
      "\n",
      "Epoch: 465 |  Train loss: 3.8067 | Val loss: 19.9612\n",
      "\n",
      "Epoch: 466 |  Train loss: 20.5160 | Val loss: 33.6936\n",
      "\n",
      "Epoch: 467 |  Train loss: 8.8820 | Val loss: 26.0397\n",
      "\n",
      "Epoch: 468 |  Train loss: 17.9497 | Val loss: 34.9122\n",
      "\n",
      "Epoch: 469 |  Train loss: 15.0385 | Val loss: 25.4554\n",
      "\n",
      "Epoch: 470 |  Train loss: 5.0676 | Val loss: 20.0022\n",
      "\n",
      "Epoch: 471 |  Train loss: 10.1243 | Val loss: 21.3483\n",
      "\n",
      "Epoch: 472 |  Train loss: 4.7787 | Val loss: 21.8611\n",
      "\n",
      "Epoch: 473 |  Train loss: 22.2227 | Val loss: 30.5756\n",
      "\n",
      "Epoch: 474 |  Train loss: 4.9915 | Val loss: 20.4423\n",
      "\n",
      "Epoch: 475 |  Train loss: 4.9897 | Val loss: 21.0631\n",
      "\n",
      "Epoch: 476 |  Train loss: 5.1219 | Val loss: 22.6057\n",
      "\n",
      "Epoch: 477 |  Train loss: 4.9308 | Val loss: 20.3645\n",
      "\n",
      "Epoch: 478 |  Train loss: 4.9436 | Val loss: 19.1966\n",
      "\n",
      "Iteration: 11000/11500 | loss = 11.8570\n",
      "Epoch: 479 |  Train loss: 8.3241 | Val loss: 22.6458\n",
      "\n",
      "Epoch: 480 |  Train loss: 5.4400 | Val loss: 21.1964\n",
      "\n",
      "Epoch: 481 |  Train loss: 4.3685 | Val loss: 20.6157\n",
      "\n",
      "Epoch: 482 |  Train loss: 4.1561 | Val loss: 19.3373\n",
      "\n",
      "Epoch: 483 |  Train loss: 3.9537 | Val loss: 20.7478\n",
      "\n",
      "Epoch: 484 |  Train loss: 7.4834 | Val loss: 21.1787\n",
      "\n",
      "Epoch: 485 |  Train loss: 9.6366 | Val loss: 20.3516\n",
      "\n",
      "Epoch: 486 |  Train loss: 10.3455 | Val loss: 30.8937\n",
      "\n",
      "Epoch: 487 |  Train loss: 18.0286 | Val loss: 35.1252\n",
      "\n",
      "Epoch: 488 |  Train loss: 8.0862 | Val loss: 21.3909\n",
      "\n",
      "Epoch: 489 |  Train loss: 4.3231 | Val loss: 18.5679\n",
      "\n",
      "Epoch: 490 |  Train loss: 13.1119 | Val loss: 31.3472\n",
      "\n",
      "Epoch: 491 |  Train loss: 5.8612 | Val loss: 25.5147\n",
      "\n",
      "Epoch: 492 |  Train loss: 7.3778 | Val loss: 25.9696\n",
      "\n",
      "Epoch: 493 |  Train loss: 4.4194 | Val loss: 22.5277\n",
      "\n",
      "Epoch: 494 |  Train loss: 6.5614 | Val loss: 22.4360\n",
      "\n",
      "Epoch: 495 |  Train loss: 5.4012 | Val loss: 23.5272\n",
      "\n",
      "Epoch: 496 |  Train loss: 7.6697 | Val loss: 29.0062\n",
      "\n",
      "Epoch: 497 |  Train loss: 26.2069 | Val loss: 52.9085\n",
      "\n",
      "Epoch: 498 |  Train loss: 5.7239 | Val loss: 26.1553\n",
      "\n",
      "Epoch: 499 |  Train loss: 4.6273 | Val loss: 22.9237\n",
      "\n",
      "Epoch: 500 |  Train loss: 8.2257 | Val loss: 21.5334\n",
      "\n",
      "\n",
      "Best Epoch: 193 | Val loss: 13.4602\n",
      "Best Parameters have been loaded in the model\n"
     ]
    }
   ],
   "source": [
    "# Best Hyperparameters found using Wandb\n",
    "config = {\n",
    "    \"parameters\": {\n",
    "        \"criterion\": \"MSE\",\n",
    "        \"model\": {\n",
    "            \"input_dim\" : 13,\n",
    "            \"activation\": \"relu\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims1\": 16,\n",
    "            \"num_classes\": 1,\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"update_rule\" : \"sgd\",\n",
    "            \"learning_rate\": 0.01,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"update_type\": \"minibatch\",\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 500,\n",
    "            \"print_every\": 1000,\n",
    "            \"calc_accuracy\": False,\n",
    "            \"verbose\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_model = trigger_training(config[\"parameters\"], X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on Test set:\n",
      "MSE Loss: 11.6963\n",
      "RMSE Loss: 3.4200\n",
      "R2 Score: 0.8052\n"
     ]
    }
   ],
   "source": [
    "# Calculating loss on Test set\n",
    "criterion = get_criterion(\"MSE\")\n",
    "\n",
    "eval_config = {\n",
    "    \"criterion\": criterion,\n",
    "    \"return_accuracy\": False,\n",
    "    \"return_loss\": True\n",
    "}\n",
    "\n",
    "best_model.eval()\n",
    "y_test_pred = best_model.forward(X_test)\n",
    "\n",
    "mse_loss = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_loss = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r_squared_score = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Metrics on Test set:\")\n",
    "print(f\"MSE Loss: {mse_loss:.4f}\")\n",
    "print(f\"RMSE Loss: {rmse_loss:.4f}\")\n",
    "print(f\"R2 Score: {r_squared_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
